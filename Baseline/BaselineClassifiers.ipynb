{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pathlib\n",
    "import random\n",
    "import datetime as dt\n",
    "import csv\n",
    "import numpy as np\n",
    "import gzip\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from nibabel.processing import conform\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBRegressor\n",
    "import scipy\n",
    "\n",
    "random.seed(1995)\n",
    "\n",
    "DATA_BASE='/home/guests/paul_hager/Projects/petgan/data'\n",
    "ADNI_DATA_BASE='/home/guests/projects/adni/data_bids_processed'\n",
    "PROJECT_BASE='/home/guests/paul_hager/Projects/ATN_Classification'\n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diag(row):\n",
    "    if (row['DXCURREN']==1) or (row['DXCHANGE']==1) or (row['DXCHANGE']==7) or (row['DXCHANGE']==9) or (row['DIAGNOSIS']==1):\n",
    "        return 'CN'\n",
    "    elif (row['DXCURREN']==2) or (row['DXCHANGE']==2) or (row['DXCHANGE']==4) or (row['DXCHANGE']==8) or (row['DIAGNOSIS']==2):\n",
    "        return 'MCI'\n",
    "    else:\n",
    "        return 'AD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPaths(pathsFile):\n",
    "    with open(pathsFile, 'r') as f:\n",
    "        paths = f.readlines()\n",
    "    paths = [p.strip() for p in paths]\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmaxNormalize(image, gmin=None, gmax=None):\n",
    "    image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zNormalize(image):\n",
    "    image = (image - np.mean(image[image != np.min(image)])) / np.std(image[image != np.min(image)])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zNormalize_torch(image):\n",
    "    image = (image - torch.mean(image[image != torch.min(image)])) / torch.std(image[image != torch.min(image)])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calc_CL_FBB(FBB_SUVR):\n",
    "    return (159.08*FBB_SUVR)-151.65\n",
    "\n",
    "def Calc_CL_AV45(AV45_SUVR):\n",
    "    return (196.9*AV45_SUVR)-196.03\n",
    "\n",
    "def Calc_FBB_CL(CL):\n",
    "    return (CL+151.65)/159.08\n",
    "\n",
    "def Calc_AV45_CL(CL):\n",
    "    return (CL+196.03)/196.9\n",
    "\n",
    "def Calc_FBB_From_AV45(AV45_SUVR):\n",
    "    return (1.2377*AV45_SUVR) - 0.279\n",
    "\n",
    "def Calc_AV45_From_FBB(FBB_SUVR):\n",
    "    return (0.808*FBB_SUVR) + 0.225393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hipp_vol(row):\n",
    "    regions = [4001, 4002, 4011, 4012, 4021, 4022, 4101, 4102, 4111, 4112, 4201, 4202] # Entire medial temporal lobe\n",
    "    regions = [4101,4102] # Hippocampus only\n",
    "    total = 0\n",
    "    for l in regions:\n",
    "        total += row[l]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "abetaDKTLabels = [1003, 1012, 1014, 1018, 1019, 1020, \n",
    "    1027, 1028, 2003, 2012, 2014, 2018, \n",
    "    2019, 2020, 2027, 2028, 1002, 1010, \n",
    "    1023, 1026, 2002, 2010, 2023, 2026, \n",
    "    1008, 1025, 1029, 1031, 2008, 2025, \n",
    "    2029, 2031, 1015, 1030, 2015, 2030]\n",
    "def Calc_Amyloid_Sensitive_SUVR_dkt(row):\n",
    "    avg_suvr = 0\n",
    "    for i in abetaDKTLabels:\n",
    "        avg_suvr += row['SUVR.DKT.ROI.idx.{}_abeta'.format(i)]\n",
    "    avg_suvr = avg_suvr / len(abetaDKTLabels)\n",
    "    return avg_suvr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted splitter that controls for bleeding\n",
    "class ContinueI(Exception):\n",
    "    pass\n",
    "\n",
    "def split_groups(out_df, num_splits=5):\n",
    "    random.seed(1995)\n",
    "    continue_i = ContinueI()\n",
    "\n",
    "    n = len(out_df)\n",
    "    all_groups = []\n",
    "    all_seen_subs = []\n",
    "    num_splits = num_splits\n",
    "    for i in range(num_splits):\n",
    "        all_groups.append([])\n",
    "        all_seen_subs.append([])\n",
    "    for indx, r in out_df.iterrows():\n",
    "        sub = r['RID']\n",
    "        try:\n",
    "            for i,seen_subs in enumerate(all_seen_subs):\n",
    "                if sub in seen_subs:\n",
    "                    all_groups[i].append(indx)\n",
    "                    raise continue_i\n",
    "        except ContinueI:\n",
    "            continue\n",
    "        if sum([len(all_groups[k]) for k in range(num_splits)]) > 0.8*n: # Last 20% should be filled acording to smallest groups for even split\n",
    "            smallest_group = float('inf')\n",
    "            i = num_splits\n",
    "            for j in range(num_splits):\n",
    "                if len(all_groups[j])<smallest_group:\n",
    "                    smallest_group = len(all_groups[j])\n",
    "                    i = j\n",
    "        else:\n",
    "            wghts = [50000*(1-len(g)/n) for g in all_groups]\n",
    "            i = random.choices(list(range(num_splits)),weights=wghts,k=1)[0]\n",
    "        all_groups[i].append(indx)\n",
    "        all_seen_subs[i].append(sub)\n",
    "\n",
    "    for g in all_groups:\n",
    "        print(len(g))\n",
    "        \n",
    "    return all_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonLabels = [2001, 2002, 2101, 2102, 2111, 2112, 2201, 2202, 2211, 2212, 2301, 2302, 2311, 2312, 2321, 2322, 2331, 2332, 2401, 2402, 2501, 2502, 2601, 2602, 2611, 2612, 2701, 2702, 3001, 3002, 4001, 4002, 4011, 4012, 4021, 4022, 4101, 4102, 4111, 4112, 4201, 4202, 5001, 5002, 5011, 5012, 5021, 5022, 5101, 5102, 5201, 5202, 5301, 5302, 5401, 5402, 6001, 6002, 6101, 6102, 6201, 6202, 6211, 6212, 6221, 6222, 6301, 6302, 6401, 6402, 7001, 7002, 7011, 7012, 7021, 7022, 7101, 7102, 8101, 8102, 8111, 8112, 8121, 8122, 8201, 8202, 8211, 8212, 8301, 8302, 9001, 9002, 9022, 9031, 9032, 9041, 9042, 9120]\n",
    "dktLabels = [2,4,5,7,8,10,11,12,13,14,15,16,17,18,24,26,28,30,31,41,43,44,46,47,49,50,51,52,53,54,58,60,62,63,77,85,251,252,253,254,255,1000,1002,1003,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1034,1035,2000,2002,2003,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2034,2035]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(model, X, y, X_test, y_test, threshold_val, _print=True):\n",
    "    start = time.time()\n",
    "    y_hat = model.predict(X)\n",
    "    print(f'Prediction time was {time.time()-start}')\n",
    "    f1 = metrics.f1_score(y, y_hat)\n",
    "    auc = metrics.roc_auc_score(y, y_hat)\n",
    "    if _print:\n",
    "        print(\"Train\")\n",
    "        print('F1: {}'.format(f1))\n",
    "        print('AUC: {}'.format(auc))\n",
    "        print(\"---\")\n",
    "\n",
    "    y_hat_test = model.decision_function(X_test)\n",
    "    auc_test = metrics.roc_auc_score(y_test, y_hat_test)\n",
    "\n",
    "    pr, re, th = metrics.precision_recall_curve(y_test,y_hat_test)\n",
    "    f1 = scipy.stats.hmean([pr,re])\n",
    "    \n",
    "    _min=1000\n",
    "    for i, t in enumerate(th):\n",
    "        if abs(t-threshold_val)<_min:\n",
    "            indx = i\n",
    "            _min = abs(t-threshold_val)\n",
    "    precision_test = pr[indx]\n",
    "    recall_test = re[indx]\n",
    "    f1_test = f1[indx]\n",
    "\n",
    "\n",
    "    if _print:\n",
    "        print(\"Test\")\n",
    "        print('F1: {} | Precision: {} | Recall: {}'.format(f1_test,precision_test,recall_test))\n",
    "        print('AUC: {}'.format(auc_test))\n",
    "    \n",
    "    return model.decision_function(X_test)>threshold_val, auc_test, f1_test, precision_test, recall_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_cutoff(y, y_hat):\n",
    "    pr, re, th = metrics.precision_recall_curve(y,y_hat)\n",
    "    f1 = scipy.stats.hmean([pr,re])\n",
    "    best_i = np.argmax(f1)\n",
    "    threshold = th[best_i]\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_atn(n, a):\n",
    "    out = []\n",
    "    for i in range(len(n)):\n",
    "        n_ = n[i]\n",
    "        a_ = a[i]\n",
    "        if n_ and a_:\n",
    "            out.append(3) # N+|A+\n",
    "        elif n_ and not a_:\n",
    "            out.append(2) # N+|A-\n",
    "        elif not n_ and a_:\n",
    "            out.append(1) # N-|A+\n",
    "        elif not n_ and not a_:\n",
    "            out.append(0) # N-|A-\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_CI(var, all_scores):\n",
    "    scores_numpy = np.array(all_scores[var])\n",
    "    mean = scores_numpy.mean()\n",
    "    CI_low = mean - 1.96 * scores_numpy.std() / np.sqrt(len(scores_numpy))\n",
    "    CI_high = mean + 1.96 * scores_numpy.std() / np.sqrt(len(scores_numpy))\n",
    "    print(f'{var:20}: {mean:.2f} [{CI_low:.2f}, {CI_high:.2f}]')\n",
    "    \n",
    "def calc_mean_std(var, all_scores):\n",
    "    scores_numpy = np.array(all_scores[var])\n",
    "    print(f'{var:20}: {scores_numpy.mean():.2f} ± {scores_numpy.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files and Tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11100 unique T1 scans from 2533 subjects\n",
      "There are 6365 unique FLAIRS from 1809 subjects\n",
      "There are 1905 unique 3D FLAIRS from 1052 subjects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4115128/2497137033.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t1_scanners_df['Scanner Manufacturer'] = t1_scanners_df.apply(lambda row: row['Imaging Protocol'].split(';')[0].split('=')[1], axis=1)\n",
      "/tmp/ipykernel_4115128/2497137033.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  t1_scanners_df['Scanner Model'] = t1_scanners_df.apply(lambda row: row['Imaging Protocol'].split(';')[1].split('=')[1], axis=1)\n"
     ]
    }
   ],
   "source": [
    "mri_df = pd.read_csv(os.path.join(DATA_BASE,'ADNI_Search','MRI_ADNI_Search.csv'))\n",
    "mri_df['Study Date'] = pd.to_datetime(mri_df['Study Date'])\n",
    "allSequences = pd.unique(mri_df['Description'])\n",
    "\n",
    "T1_Sequences = [v for i,v in enumerate(allSequences) if ('mprage' in v.lower() or 'mp-rage' in v.lower() or 'spgr' in v.lower()) and 'repe' not in v.lower()]\n",
    "t1_df_dup = mri_df[mri_df['Description'].isin(T1_Sequences)]\n",
    "t1_df = t1_df_dup.drop_duplicates(subset=['Subject ID','Study Date'])\n",
    "t1_scanners_df = t1_df[t1_df['Imaging Protocol'].str.contains('Mfg Model').notna()]\n",
    "t1_scanners_df['Scanner Manufacturer'] = t1_scanners_df.apply(lambda row: row['Imaging Protocol'].split(';')[0].split('=')[1], axis=1)\n",
    "t1_scanners_df['Scanner Model'] = t1_scanners_df.apply(lambda row: row['Imaging Protocol'].split(';')[1].split('=')[1], axis=1)\n",
    "\n",
    "print(\"There are {} unique T1 scans from {} subjects\".format(len(t1_df),len(pd.unique(t1_df['Subject ID']))))\n",
    "\n",
    "FLAIR_Sequences = [v for i,v in enumerate(allSequences) if 'flair' in v.lower()]\n",
    "FLAIR_Sequences_3D = [v for v in FLAIR_Sequences if '3d' in v.lower()]\n",
    "flair_df_dup = mri_df[mri_df['Description'].isin(FLAIR_Sequences)]\n",
    "flair_df = flair_df_dup.drop_duplicates(subset=['Subject ID','Study Date'])\n",
    "mri_3D_flair_df_dup = mri_df[mri_df['Description'].isin(FLAIR_Sequences_3D)]\n",
    "mri_3D_flair_df = mri_3D_flair_df_dup.drop_duplicates(subset=['Subject ID','Study Date'])\n",
    "print(\"There are {} unique FLAIRS from {} subjects\".format(len(flair_df),len(pd.unique(flair_df['Subject ID']))))\n",
    "print(\"There are {} unique 3D FLAIRS from {} subjects\".format(len(mri_3D_flair_df),len(pd.unique(mri_3D_flair_df['Subject ID']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1373 unique tau scans from 870 subjects\n",
      "There are 3529 unique abeta scans from 1677 subjects\n",
      "There are 3737 unique fdg scans from 1662 subjects\n"
     ]
    }
   ],
   "source": [
    "pet_df = pd.read_csv(os.path.join(DATA_BASE,'ADNI_Search','PET_ADNI_Search.csv'))\n",
    "pet_df['Study Date'] = pd.to_datetime(pet_df['Study Date'])\n",
    "allSequences_PET = pd.unique(pet_df['Imaging Protocol'])\n",
    "\n",
    "tau_sequences = [v for i,v in enumerate(allSequences_PET) if 'av1451' in v.lower()]\n",
    "tau_df_dup = pet_df[pet_df['Imaging Protocol'].isin(tau_sequences)]\n",
    "tau_df = tau_df_dup.drop_duplicates(subset=['Subject ID','Study Date'])\n",
    "print(\"There are {} unique tau scans from {} subjects\".format(len(tau_df),len(pd.unique(tau_df['Subject ID']))))\n",
    "\n",
    "abeta_sequences = [v for i,v in enumerate(allSequences_PET) if 'av45' in v.lower() or 'fbb' in v.lower()]\n",
    "abeta_df_dup = pet_df[pet_df['Imaging Protocol'].isin(abeta_sequences)]\n",
    "abeta_df = abeta_df_dup.drop_duplicates(subset=['Subject ID','Study Date'])\n",
    "print(\"There are {} unique abeta scans from {} subjects\".format(len(abeta_df),len(pd.unique(abeta_df['Subject ID']))))\n",
    "\n",
    "fdg_sequences = [v for i,v in enumerate(allSequences_PET) if 'fdg' in v.lower()]\n",
    "fdg_df_dup = pet_df[pet_df['Imaging Protocol'].isin(fdg_sequences)]\n",
    "fdg_df = fdg_df_dup.drop_duplicates(subset=['Subject ID','Study Date'])\n",
    "print(\"There are {} unique fdg scans from {} subjects\".format(len(fdg_df),len(pd.unique(fdg_df['Subject ID']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12842 unique diagnoses from 2853 subjects\n"
     ]
    }
   ],
   "source": [
    "diag_df = pd.read_csv(os.path.join(DATA_BASE,'ADNI_Search','DXSUM_PDXCONV_ADNIALL.csv'))\n",
    "diag_df['EXAMDATE'] = pd.to_datetime(diag_df['EXAMDATE'])\n",
    "diag_df = diag_df.rename(columns={'EXAMDATE':'Study Date', 'PTID':'Subject ID'})\n",
    "diag_df = diag_df.dropna(subset=['Study Date'])\n",
    "print(\"There are {} unique diagnoses from {} subjects\".format(len(diag_df),len(pd.unique(diag_df['RID']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3115 unique biomark readings from 1613 subjects\n"
     ]
    }
   ],
   "source": [
    "biomark1_df = pd.read_csv(os.path.join(DATA_BASE,'ADNI_Search','UPENNBIOMK9_04_19_17.csv'), usecols=['RID', 'EXAMDATE', 'ABETA', 'PTAU', 'TAU'])\n",
    "biomark1_df = biomark1_df.rename(columns={'ABETA':'ABETA42'})\n",
    "biomark2_df = pd.read_csv(os.path.join(DATA_BASE,'ADNI_Search','UPENNBIOMK10_07_29_19.csv'), usecols=['RID', 'DRAWDATE', 'ABETA40', 'ABETA42', 'PTAU', 'TAU'])\n",
    "biomark2_df = biomark2_df.rename(columns={'DRAWDATE':'EXAMDATE'})\n",
    "biomark3_df = pd.read_csv(os.path.join(DATA_BASE,'ADNI_Search','UPENNBIOMK12_01_04_21.csv'), usecols=['RID', 'EXAMDATE', 'AB40', 'ABETA', 'PTAU', 'TAU'])\n",
    "biomark3_df = biomark3_df.rename(columns={'AB40':'ABETA40','ABETA':'ABETA42'})\n",
    "\n",
    "biomark_df = pd.concat([biomark1_df,biomark2_df,biomark3_df])\n",
    "biomark_df = biomark_df.reset_index(drop=True)\n",
    "biomark_df = biomark_df.rename(columns={'EXAMDATE':'CSF Date'})\n",
    "biomark_df['CSF Date'] = pd.to_datetime(biomark_df['CSF Date'])\n",
    "biomark_df['RID'] = pd.to_numeric(biomark_df['RID'])\n",
    "biomark_df = biomark_df.replace('>120','121')\n",
    "biomark_df = biomark_df.replace('<8','7')\n",
    "biomark_df = biomark_df.replace('>1700','1701')\n",
    "biomark_df = biomark_df.replace('>1300','1301')\n",
    "biomark_df = biomark_df.replace('<200','199')\n",
    "biomark_df = biomark_df.replace('<80','80')\n",
    "biomark_df['PTAU'] = pd.to_numeric(biomark_df['PTAU'])\n",
    "biomark_df['TAU'] = pd.to_numeric(biomark_df['TAU'])\n",
    "biomark_df['ABETA40'] = pd.to_numeric(biomark_df['ABETA40'])\n",
    "biomark_df['ABETA42'] = pd.to_numeric(biomark_df['ABETA42'])\n",
    "print(\"There are {} unique biomark readings from {} subjects\".format(len(biomark_df),len(pd.unique(biomark_df['RID']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find overlap between dataframes\n",
    "def get_overlap_dataframe(df1,df2,df1_name='1',df2_name='2',df1_date_name='Study Date',df2_date_name='Study Date',df1_id_name='Subject ID',df2_id_name='Subject ID',days_delta=183):\n",
    "    df1_to_df2_index = {}\n",
    "    for indx, df1_r in df1.iterrows():\n",
    "        df2_filt = df2[df2[df2_id_name]==df1_r[df1_id_name]]\n",
    "        bestDif = dt.timedelta(days=99999)\n",
    "        for indx2, df2_r in df2_filt.iterrows():\n",
    "            dif = abs(df1_r[df1_date_name]-df2_r[df2_date_name])\n",
    "            if dif<dt.timedelta(days=days_delta) and dif<bestDif:\n",
    "                bestDif = dif\n",
    "                df1_to_df2_index[indx] = indx2\n",
    "    df1_filtered_df = df1.loc[list(df1_to_df2_index.keys())]\n",
    "    df2_filtered_df = df2.loc[list(df1_to_df2_index.values())]\n",
    "    for c in ['Study Date','Description','Imaging Protocol','Image ID']:\n",
    "        df1_filtered_df = df1_filtered_df.rename(columns={c:'{}_{}'.format(c,df1_name)})\n",
    "        df2_filtered_df = df2_filtered_df.rename(columns={c:'{}_{}'.format(c,df2_name)})\n",
    "    overlap_df = pd.concat([df1_filtered_df.reset_index(drop=True),df2_filtered_df.reset_index(drop=True)], axis=1)\n",
    "    #for i in [4,6,7,8]:\n",
    "    #    overlap_df.columns.values[i] = '{}_{}'.format(overlap_df.columns.values[i],df1_name)\n",
    "    #for i in [13,15,16,17]:\n",
    "    #    overlap_df.columns.values[i] = '{}_{}'.format(overlap_df.columns.values[i],df2_name)\n",
    "    overlap_df = overlap_df.loc[:,~overlap_df.columns.duplicated()]\n",
    "    return overlap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_overlap_dataframe(df1,df2,df1_date_name='Study Date',df2_date_name='Study Date',days_delta=183):\n",
    "t1_tau_overlap_df = get_overlap_dataframe(t1_df,tau_df,df1_name='t1',df2_name='tau')\n",
    "t1_abeta_overlap_df = get_overlap_dataframe(t1_df,abeta_df,df1_name='t1',df2_name='abeta')\n",
    "t1_fdg_overlap_df = get_overlap_dataframe(t1_df,fdg_df,df1_name='t1',df2_name='fdg')\n",
    "tau_abeta_overlap_df = get_overlap_dataframe(tau_df,abeta_df,df1_name='tau',df2_name='abeta')\n",
    "tau_fdg_overlap_df = get_overlap_dataframe(tau_df,fdg_df,df1_name='tau',df2_name='fdg')\n",
    "abeta_fdg_overlap_df = get_overlap_dataframe(abeta_df,fdg_df,df1_name='abeta',df2_name='fdg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between t1 and tau is 1232 scans from 803 subjects\n",
      "Overlap between t1 and abeta is 4534 scans from 1635 subjects\n",
      "Overlap between t1 and fdg is 4827 scans from 1637 subjects\n",
      "Overlap between tau and abeta is 1072 scans from 823 subjects\n",
      "Overlap between tau and fdg is 339 scans from 339 subjects\n",
      "Overlap between abeta and fdg is 1917 scans from 1320 subjects\n"
     ]
    }
   ],
   "source": [
    "print(\"Overlap between {} and {} is {} scans from {} subjects\".format('t1','tau',len(t1_tau_overlap_df),len(pd.unique(t1_tau_overlap_df['Subject ID']))))\n",
    "print(\"Overlap between {} and {} is {} scans from {} subjects\".format('t1','abeta',len(t1_abeta_overlap_df),len(pd.unique(t1_abeta_overlap_df['Subject ID']))))\n",
    "print(\"Overlap between {} and {} is {} scans from {} subjects\".format('t1','fdg',len(t1_fdg_overlap_df),len(pd.unique(t1_fdg_overlap_df['Subject ID']))))\n",
    "print(\"Overlap between {} and {} is {} scans from {} subjects\".format('tau','abeta',len(tau_abeta_overlap_df),len(pd.unique(tau_abeta_overlap_df['Subject ID']))))\n",
    "print(\"Overlap between {} and {} is {} scans from {} subjects\".format('tau','fdg',len(tau_fdg_overlap_df),len(pd.unique(tau_fdg_overlap_df['Subject ID']))))\n",
    "print(\"Overlap between {} and {} is {} scans from {} subjects\".format('abeta','fdg',len(abeta_fdg_overlap_df),len(pd.unique(abeta_fdg_overlap_df['Subject ID']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_tau_abeta_overlap_df = get_overlap_dataframe(t1_tau_overlap_df,abeta_df,df1_name='tau',df2_name='abeta',df1_date_name='Study Date_t1')\n",
    "t1_tau_fdg_overlap_df = get_overlap_dataframe(t1_tau_overlap_df,fdg_df,df1_name='tau',df2_name='fdg',df1_date_name='Study Date_t1')\n",
    "t1_abeta_fdg_overlap_df = get_overlap_dataframe(t1_abeta_overlap_df,fdg_df,df1_name='abeta',df2_name='fdg',df1_date_name='Study Date_t1')\n",
    "tau_abeta_fdg_overlap_df = get_overlap_dataframe(tau_abeta_overlap_df,fdg_df,df1_name='abeta',df2_name='fdg',df1_date_name='Study Date_tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between t1 and tau and abeta is 980 scans from 766 subjects\n",
      "Overlap between t1 and tau and fdg is 312 scans from 311 subjects\n",
      "Overlap between t1 and abeta and fdg is 3001 scans from 1289 subjects\n",
      "Overlap between tau and abeta and fdg is 329 scans from 329 subjects\n"
     ]
    }
   ],
   "source": [
    "print(\"Overlap between {} and {} and {} is {} scans from {} subjects\".format('t1','tau','abeta',len(t1_tau_abeta_overlap_df),len(pd.unique(t1_tau_abeta_overlap_df['Subject ID']))))\n",
    "print(\"Overlap between {} and {} and {} is {} scans from {} subjects\".format('t1','tau','fdg',len(t1_tau_fdg_overlap_df),len(pd.unique(t1_tau_fdg_overlap_df['Subject ID']))))\n",
    "print(\"Overlap between {} and {} and {} is {} scans from {} subjects\".format('t1','abeta','fdg',len(t1_abeta_fdg_overlap_df),len(pd.unique(t1_abeta_fdg_overlap_df['Subject ID']))))\n",
    "print(\"Overlap between {} and {} and {} is {} scans from {} subjects\".format('tau','abeta','fdg',len(tau_abeta_fdg_overlap_df),len(pd.unique(tau_abeta_fdg_overlap_df['Subject ID']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between t1 and abeta is 8383 scans from 1960 subjects\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "biomark_df = biomark_df.rename(columns={'CSF Date':'Study Date'})\n",
    "biomark_df['RID'] = biomark_df.apply(lambda row: str(row['RID']).zfill(4),axis=1)\n",
    "abeta_df['RID'] = abeta_df.apply(lambda row: row['Subject ID'].split('_')[2], axis=1)\n",
    "t1_df['RID'] = t1_df.apply(lambda row: row['Subject ID'].split('_')[2], axis=1)\n",
    "abeta_pet_csf_df = pd.concat([abeta_df,biomark_df])\n",
    "t1_abeta_pet_csf_overlap_df = get_overlap_dataframe(t1_df,abeta_pet_csf_df,df1_name='t1',df2_name='abeta',df1_id_name='RID',df2_id_name='RID')\n",
    "print(\"Overlap between {} and {} is {} scans from {} subjects\".format('t1','abeta',len(t1_abeta_pet_csf_overlap_df),len(pd.unique(t1_abeta_pet_csf_overlap_df['Subject ID']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_t1_overlap_df = get_overlap_dataframe(t1_df,diag_df,df2_name='diag',df1_name='t1')\n",
    "diag_tau_overlap_df = get_overlap_dataframe(tau_df,diag_df,df2_name='diag',df1_name='tau')\n",
    "diag_abeta_overlap_df = get_overlap_dataframe(abeta_df,diag_df,df2_name='diag',df1_name='abeta')\n",
    "diag_fdg_overlap_df = get_overlap_dataframe(fdg_df,diag_df,df2_name='diag',df1_name='fdg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between diag and t1 is 10864 scans from 2487 subjects\n",
      "Overlap between diag and tau is 1300 scans from 842 subjects\n",
      "Overlap between diag and abeta is 3418 scans from 1669 subjects\n",
      "Overlap between diag and fdg is 3691 scans from 1656 subjects\n"
     ]
    }
   ],
   "source": [
    "print(\"Overlap between {} and {} is {} scans from {} subjects\".format('diag','t1',len(diag_t1_overlap_df),len(pd.unique(diag_t1_overlap_df['Subject ID']))))\n",
    "print(\"Overlap between {} and {} is {} scans from {} subjects\".format('diag','tau',len(diag_tau_overlap_df),len(pd.unique(diag_tau_overlap_df['Subject ID']))))\n",
    "print(\"Overlap between {} and {} is {} scans from {} subjects\".format('diag','abeta',len(diag_abeta_overlap_df),len(pd.unique(diag_abeta_overlap_df['Subject ID']))))\n",
    "print(\"Overlap between {} and {} is {} scans from {} subjects\".format('diag','fdg',len(diag_fdg_overlap_df),len(pd.unique(diag_fdg_overlap_df['Subject ID']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Tau and Amyloid from PET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "suvr_df = pd.read_csv(os.path.join(DATA_BASE,'ADNI_Tau_Amyloid_SUVR_status.csv'))\n",
    "suvr_df['RID'] = pd.to_numeric(suvr_df['RID'])\n",
    "suvr_df['acq.date'] = pd.to_datetime(suvr_df['acq.date'])\n",
    "#tau_abeta_overlap_copy = tau_abeta_overlap_df.loc[~tau_abeta_overlap_df.duplicated(subset=['Subject ID'],keep='first')]\n",
    "tau_abeta_overlap_copy = tau_abeta_overlap_df.copy(deep=True)\n",
    "tau_abeta_overlap_copy['RID'] = tau_abeta_overlap_copy.apply(lambda row: int(row['Subject ID'].split('_')[2]), axis=1)\n",
    "tau_abeta_overlap_suvr_df = tau_abeta_overlap_copy.merge(suvr_df[suvr_df['pet.modality']=='pet-AV1451'], how='left', left_on=['RID','Study Date_tau'], right_on=['RID','acq.date'])\n",
    "tau_abeta_overlap_suvr_df = tau_abeta_overlap_suvr_df.merge(suvr_df[(suvr_df['pet.modality']=='pet-AV45') | (suvr_df['pet.modality']=='pet-FBB')], how='left', left_on=['RID','Study Date_abeta'], right_on=['RID','acq.date'], suffixes=['_tau','_abeta'])\n",
    "subset = ['SUVR.DKT.ROI.idx.{}_tau'.format(x) for x in dktLabels]\n",
    "subset.extend(['SUVR.DKT.ROI.idx.{}_abeta'.format(x) for x in dktLabels])\n",
    "subset.append('APOE A1')\n",
    "tau_abeta_overlap_suvr_df = tau_abeta_overlap_suvr_df.dropna(subset=subset)\n",
    "tau_abeta_overlap_suvr_df['apoe'] = tau_abeta_overlap_suvr_df.apply(lambda row: '{}{}'.format(int(row['APOE A1']),int(row['APOE A2'])), axis=1)\n",
    "tau_abeta_overlap_suvr_df = tau_abeta_overlap_suvr_df.dropna(subset=subset)\n",
    "out_df = tau_abeta_overlap_suvr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_tau_abeta_overlap_df = get_overlap_dataframe(tau_abeta_overlap_suvr_df,diag_df,df1_date_name='Study Date_tau',days_delta=1265)\n",
    "diag_tau_abeta_overlap_df['Diag'] = diag_tau_abeta_overlap_df.apply(lambda row: get_diag(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CN     466\n",
       "MCI    200\n",
       "AD      70\n",
       "Name: Diag, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_tau_abeta_overlap_df['Diag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   T+  count\n",
      "0   0    689\n",
      "1   1     50\n",
      "   A+  count\n",
      "0   0    324\n",
      "1   1    200\n",
      "   A+  count\n",
      "0   0    135\n",
      "1   1     80\n"
     ]
    }
   ],
   "source": [
    "tau_abeta_overlap_suvr_df['T+'] = tau_abeta_overlap_suvr_df.apply(lambda row: 1 if row['tau.global.SUVR_tau']>1.3 else 0, axis=1)\n",
    "print(tau_abeta_overlap_suvr_df.groupby([\"T+\"]).size().reset_index().rename(columns={0:'count'}))\n",
    "\n",
    "tau_abeta_AV45_overlap_suvr_df = tau_abeta_overlap_suvr_df[(tau_abeta_overlap_suvr_df['Description_abeta'].str.contains(\"AV45\"))|(tau_abeta_overlap_suvr_df['Description_abeta'].str.contains(\"AV_45\"))|(tau_abeta_overlap_suvr_df['Description_abeta'].str.contains(\"AV-45\"))]\n",
    "tau_abeta_FBB_overlap_suvr_df = tau_abeta_overlap_suvr_df[(tau_abeta_overlap_suvr_df['Description_abeta'].str.contains(\"FBB\"))&~(tau_abeta_overlap_suvr_df['Description_abeta'].str.contains(\"AV45\"))]\n",
    "\n",
    "out_df = tau_abeta_FBB_overlap_suvr_df\n",
    "\n",
    "# Calc A status\n",
    "tau_abeta_AV45_overlap_suvr_df['Amyloid Sensitive SUVR'] = tau_abeta_AV45_overlap_suvr_df.apply(lambda row: Calc_Amyloid_Sensitive_SUVR_dkt(row), axis=1)\n",
    "av45_cutoff = 1.11\n",
    "fbb_cutoff = 1.08\n",
    "tau_abeta_AV45_overlap_suvr_df['A+'] = tau_abeta_AV45_overlap_suvr_df.apply(lambda row: 1 if row['Amyloid Sensitive SUVR']>av45_cutoff else 0, axis=1)\n",
    "print(tau_abeta_AV45_overlap_suvr_df.groupby([\"A+\"]).size().reset_index().rename(columns={0:'count'}))\n",
    "\n",
    "tau_abeta_FBB_overlap_suvr_df['Amyloid Sensitive SUVR'] = tau_abeta_FBB_overlap_suvr_df.apply(lambda row: Calc_Amyloid_Sensitive_SUVR_dkt(row), axis=1)\n",
    "av45_cutoff = 1.11\n",
    "fbb_cutoff = 1.08\n",
    "tau_abeta_FBB_overlap_suvr_df['A+'] = tau_abeta_FBB_overlap_suvr_df.apply(lambda row: 1 if row['Amyloid Sensitive SUVR']>fbb_cutoff else 0, axis=1)\n",
    "print(tau_abeta_FBB_overlap_suvr_df.groupby([\"A+\"]).size().reset_index().rename(columns={0:'count'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids_amyloid = []\n",
    "for indx, row in out_df.iterrows():\n",
    "    test_ids_amyloid.append('{}|{}'.format(row['RID'],row['Study Date_tau']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calc Population Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_abeta_AV45_overlap_suvr_df_diag = get_overlap_dataframe(tau_abeta_AV45_overlap_suvr_df, diag_df, df1_date_name='Study Date_tau')\n",
    "tau_abeta_AV45_overlap_suvr_df_diag['diag'] = tau_abeta_AV45_overlap_suvr_df_diag.apply(lambda x: get_diag(x),axis=1)\n",
    "\n",
    "tau_abeta_FBB_overlap_suvr_df_diag = get_overlap_dataframe(tau_abeta_FBB_overlap_suvr_df, diag_df, df1_date_name='Study Date_tau', days_delta=600)\n",
    "tau_abeta_FBB_overlap_suvr_df_diag['diag'] = tau_abeta_FBB_overlap_suvr_df_diag.apply(lambda x: get_diag(x),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject ID</th>\n",
       "      <th>Sex</th>\n",
       "      <th>APOE A1</th>\n",
       "      <th>APOE A2</th>\n",
       "      <th>Study Date_tau</th>\n",
       "      <th>Age</th>\n",
       "      <th>Description_tau</th>\n",
       "      <th>Imaging Protocol_tau</th>\n",
       "      <th>Image ID_tau</th>\n",
       "      <th>Study Date_abeta</th>\n",
       "      <th>...</th>\n",
       "      <th>t.diff.adas.pet.yrs_abeta</th>\n",
       "      <th>PHASE_abeta</th>\n",
       "      <th>DX_abeta</th>\n",
       "      <th>SITEID_abeta</th>\n",
       "      <th>t.diff.diagnosis.pet.yrs_abeta</th>\n",
       "      <th>Phase_abeta</th>\n",
       "      <th>apoe</th>\n",
       "      <th>T+</th>\n",
       "      <th>Amyloid Sensitive SUVR</th>\n",
       "      <th>A+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>006_S_6209</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-02-20</td>\n",
       "      <td>72.3</td>\n",
       "      <td>Brain_ADNI3_AV1451_LM (AC) Tau</td>\n",
       "      <td>Radiopharmaceutical=18F-AV1451;Mfg Model=Biogr...</td>\n",
       "      <td>966777</td>\n",
       "      <td>2018-02-27</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.990054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>006_S_6234</td>\n",
       "      <td>F</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-03-22</td>\n",
       "      <td>70.7</td>\n",
       "      <td>Brain_ADNI3_AV1451_NEWATN1-2 (AC) Tau</td>\n",
       "      <td>Radiopharmaceutical=18F-AV1451;Mfg Model=Biogr...</td>\n",
       "      <td>980183</td>\n",
       "      <td>2018-03-06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1.046531</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>006_S_6243</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>66.9</td>\n",
       "      <td>Brain_ADNI3_AV1451_LM (AC) Tau</td>\n",
       "      <td>Radiopharmaceutical=18F-AV1451;Mfg Model=Biogr...</td>\n",
       "      <td>981657</td>\n",
       "      <td>2018-03-14</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MCI</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.043836</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.869037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>006_S_6252</td>\n",
       "      <td>F</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>74.8</td>\n",
       "      <td>Brain_ADNI3_AV1451_LM (AC) Tau</td>\n",
       "      <td>Radiopharmaceutical=18F-AV1451;Mfg Model=Biogr...</td>\n",
       "      <td>989600</td>\n",
       "      <td>2018-04-10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MCI</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.063014</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1.845195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>006_S_6277</td>\n",
       "      <td>F</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-05-07</td>\n",
       "      <td>70.9</td>\n",
       "      <td>Brain_ADNI3_AV1451_LM (AC) Tau</td>\n",
       "      <td>Radiopharmaceutical=18F-AV1451;Mfg Model=Biogr...</td>\n",
       "      <td>994547</td>\n",
       "      <td>2018-04-17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.043836</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.899692</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>941_S_6575</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-10-24</td>\n",
       "      <td>73.6</td>\n",
       "      <td>ADNI3-BRAIN Tau</td>\n",
       "      <td>Radiopharmaceutical=18F-AV1451;Mfg Model=Disco...</td>\n",
       "      <td>1064703</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-0.115068</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1.277207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>941_S_6575</td>\n",
       "      <td>M</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2021-01-20</td>\n",
       "      <td>75.9</td>\n",
       "      <td>ADNI3-BRAIN Tau</td>\n",
       "      <td>Radiopharmaceutical=18F-AV1451;Mfg Model=Disco...</td>\n",
       "      <td>1403267</td>\n",
       "      <td>2020-11-25</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-0.079452</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1.382339</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>941_S_6580</td>\n",
       "      <td>F</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>80.0</td>\n",
       "      <td>ADNI3-BRAIN Tau</td>\n",
       "      <td>Radiopharmaceutical=18F-AV1451;Mfg Model=Disco...</td>\n",
       "      <td>1065699</td>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-0.205479</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1.408058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>941_S_6580</td>\n",
       "      <td>F</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2020-10-09</td>\n",
       "      <td>82.0</td>\n",
       "      <td>ADNI3-BRAIN Tau</td>\n",
       "      <td>Radiopharmaceutical=18F-AV1451;Mfg Model=Disco...</td>\n",
       "      <td>1348905</td>\n",
       "      <td>2020-10-20</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1.485173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>941_S_6581</td>\n",
       "      <td>F</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>74.5</td>\n",
       "      <td>ADNI3-BRAIN Tau</td>\n",
       "      <td>Radiopharmaceutical=18F-AV1451;Mfg Model=Disco...</td>\n",
       "      <td>1051615</td>\n",
       "      <td>2018-09-19</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.016438</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1.019953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215 rows × 704 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Subject ID Sex  APOE A1  APOE A2 Study Date_tau   Age  \\\n",
       "60    006_S_6209   M      3.0      3.0     2018-02-20  72.3   \n",
       "61    006_S_6234   F      3.0      4.0     2018-03-22  70.7   \n",
       "62    006_S_6243   M      3.0      3.0     2018-04-04  66.9   \n",
       "63    006_S_6252   F      3.0      4.0     2018-04-25  74.8   \n",
       "64    006_S_6277   F      3.0      3.0     2018-05-07  70.9   \n",
       "...          ...  ..      ...      ...            ...   ...   \n",
       "1064  941_S_6575   M      3.0      3.0     2018-10-24  73.6   \n",
       "1065  941_S_6575   M      3.0      3.0     2021-01-20  75.9   \n",
       "1066  941_S_6580   F      3.0      3.0     2018-10-26  80.0   \n",
       "1067  941_S_6580   F      3.0      3.0     2020-10-09  82.0   \n",
       "1068  941_S_6581   F      3.0      3.0     2018-09-21  74.5   \n",
       "\n",
       "                            Description_tau  \\\n",
       "60           Brain_ADNI3_AV1451_LM (AC) Tau   \n",
       "61    Brain_ADNI3_AV1451_NEWATN1-2 (AC) Tau   \n",
       "62           Brain_ADNI3_AV1451_LM (AC) Tau   \n",
       "63           Brain_ADNI3_AV1451_LM (AC) Tau   \n",
       "64           Brain_ADNI3_AV1451_LM (AC) Tau   \n",
       "...                                     ...   \n",
       "1064                        ADNI3-BRAIN Tau   \n",
       "1065                        ADNI3-BRAIN Tau   \n",
       "1066                        ADNI3-BRAIN Tau   \n",
       "1067                        ADNI3-BRAIN Tau   \n",
       "1068                        ADNI3-BRAIN Tau   \n",
       "\n",
       "                                   Imaging Protocol_tau  Image ID_tau  \\\n",
       "60    Radiopharmaceutical=18F-AV1451;Mfg Model=Biogr...        966777   \n",
       "61    Radiopharmaceutical=18F-AV1451;Mfg Model=Biogr...        980183   \n",
       "62    Radiopharmaceutical=18F-AV1451;Mfg Model=Biogr...        981657   \n",
       "63    Radiopharmaceutical=18F-AV1451;Mfg Model=Biogr...        989600   \n",
       "64    Radiopharmaceutical=18F-AV1451;Mfg Model=Biogr...        994547   \n",
       "...                                                 ...           ...   \n",
       "1064  Radiopharmaceutical=18F-AV1451;Mfg Model=Disco...       1064703   \n",
       "1065  Radiopharmaceutical=18F-AV1451;Mfg Model=Disco...       1403267   \n",
       "1066  Radiopharmaceutical=18F-AV1451;Mfg Model=Disco...       1065699   \n",
       "1067  Radiopharmaceutical=18F-AV1451;Mfg Model=Disco...       1348905   \n",
       "1068  Radiopharmaceutical=18F-AV1451;Mfg Model=Disco...       1051615   \n",
       "\n",
       "     Study Date_abeta  ... t.diff.adas.pet.yrs_abeta PHASE_abeta  DX_abeta  \\\n",
       "60         2018-02-27  ...                       NaN         NaN        CN   \n",
       "61         2018-03-06  ...                       NaN         NaN        CN   \n",
       "62         2018-03-14  ...                       NaN         NaN       MCI   \n",
       "63         2018-04-10  ...                       NaN         NaN       MCI   \n",
       "64         2018-04-17  ...                       NaN         NaN        CN   \n",
       "...               ...  ...                       ...         ...       ...   \n",
       "1064       2018-10-30  ...                       NaN         NaN        CN   \n",
       "1065       2020-11-25  ...                       NaN         NaN        CN   \n",
       "1066       2018-11-28  ...                       NaN         NaN        CN   \n",
       "1067       2020-10-20  ...                       NaN         NaN        CN   \n",
       "1068       2018-09-19  ...                       NaN         NaN        CN   \n",
       "\n",
       "      SITEID_abeta t.diff.diagnosis.pet.yrs_abeta  Phase_abeta apoe T+  \\\n",
       "60             4.0                      -0.013699        ADNI3   33  0   \n",
       "61             4.0                       0.002740        ADNI3   34  0   \n",
       "62             4.0                      -0.043836        ADNI3   33  0   \n",
       "63             4.0                       0.063014        ADNI3   34  1   \n",
       "64             4.0                       0.043836        ADNI3   33  0   \n",
       "...            ...                            ...          ...  ... ..   \n",
       "1064          59.0                      -0.115068        ADNI3   33  0   \n",
       "1065          59.0                      -0.079452        ADNI3   33  0   \n",
       "1066          59.0                      -0.205479        ADNI3   33  0   \n",
       "1067          59.0                       0.005479        ADNI3   33  0   \n",
       "1068          59.0                       0.016438        ADNI3   33  0   \n",
       "\n",
       "     Amyloid Sensitive SUVR A+  \n",
       "60                 0.990054  0  \n",
       "61                 1.046531  0  \n",
       "62                 0.869037  0  \n",
       "63                 1.845195  1  \n",
       "64                 0.899692  0  \n",
       "...                     ... ..  \n",
       "1064               1.277207  1  \n",
       "1065               1.382339  1  \n",
       "1066               1.408058  1  \n",
       "1067               1.485173  1  \n",
       "1068               1.019953  0  \n",
       "\n",
       "[215 rows x 704 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_abeta_FBB_overlap_suvr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60      6209\n",
       "61      6234\n",
       "62      6243\n",
       "63      6252\n",
       "64      6277\n",
       "        ... \n",
       "1064    6575\n",
       "1065    6575\n",
       "1066    6580\n",
       "1067    6580\n",
       "1068    6581\n",
       "Name: RID, Length: 215, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_abeta_FBB_overlap_suvr_df['RID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PROJECT_BASE,'data/splits_FBB_as_AV45_multitask/dkt/split_5_ids.csv'), 'w') as f:\n",
    "        out = 'RID,Study Date Tau,Study Date Abeta,Diagnosis\\n'\n",
    "        f.write(out)\n",
    "        for indx, row in tau_abeta_FBB_overlap_suvr_df_diag.iterrows():\n",
    "            out = f'{row[\"RID\"]},{row[\"Study Date_tau\"]},{row[\"Study Date_abeta\"]},{row[\"diag\"]}\\n'\n",
    "            f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    diag\n",
       "0     CN\n",
       "1     CN\n",
       "2    MCI\n",
       "3    MCI\n",
       "4     CN\n",
       "..   ...\n",
       "210   CN\n",
       "211   CN\n",
       "212   CN\n",
       "213   CN\n",
       "214   CN\n",
       "\n",
       "[215 rows x 1 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 1000)\n",
    "tau_abeta_FBB_overlap_suvr_df_diag[['diag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AV45 N=502\n",
      "Test FBB N=212\n",
      "---\n",
      "For Diagnosis CN (N=312)\n",
      "Sex (f/m): 178/134\n",
      "Age: 75.42 ± 7.16\n",
      "Global Tau: 1.07 ± 0.09\n",
      "Global Amyloid: 1.06 ± 0.20\n",
      "Amyloid Status (+/-): 87/225\n",
      "Tau Status (+/-): 5/307\n",
      "MMSE: 29.02 ± 1.30\n",
      "ADAS13: 7.64 ± 4.36\n",
      "APOE Status (+/-): 94/165\n",
      "---\n",
      "For Diagnosis MCI (N=145)\n",
      "Sex (f/m): 56/89\n",
      "Age: 77.64 ± 7.28\n",
      "Global Tau: 1.12 ± 0.15\n",
      "Global Amyloid: 1.14 ± 0.28\n",
      "Amyloid Status (+/-): 65/80\n",
      "Tau Status (+/-): 9/136\n",
      "MMSE: 28.03 ± 2.04\n",
      "ADAS13: 13.39 ± 6.46\n",
      "APOE Status (+/-): 35/81\n",
      "---\n",
      "For Diagnosis AD (N=45)\n",
      "Sex (f/m): 19/26\n",
      "Age: 78.74 ± 9.49\n",
      "Global Tau: 1.27 ± 0.36\n",
      "Global Amyloid: 1.36 ± 0.24\n",
      "Amyloid Status (+/-): 40/5\n",
      "Tau Status (+/-): 14/31\n",
      "MMSE: 21.63 ± 4.94\n",
      "ADAS13: 29.63 ± 10.85\n",
      "APOE Status (+/-): 14/23\n",
      "---\n",
      "@@@@\n",
      "For Diagnosis CN (N=144)\n",
      "Sex (f/m): 88/56\n",
      "Age: 71.18 ± 5.93\n",
      "Global Tau: 1.08 ± 0.10\n",
      "Global Amyloid: 1.06 ± 0.22\n",
      "Amyloid Status (+/-): 44/100\n",
      "Tau Status (+/-): 5/139\n",
      "MMSE: 29.54 ± 0.78\n",
      "ADAS13: 8.44 ± 4.89\n",
      "APOE Status (+/-): 52/77\n",
      "---\n",
      "For Diagnosis MCI (N=46)\n",
      "Sex (f/m): 20/26\n",
      "Age: 71.24 ± 8.07\n",
      "Global Tau: 1.13 ± 0.19\n",
      "Global Amyloid: 1.12 ± 0.32\n",
      "Amyloid Status (+/-): 15/31\n",
      "Tau Status (+/-): 5/41\n",
      "MMSE: 27.50 ± 1.91\n",
      "ADAS13: 12.50 ± 0.84\n",
      "APOE Status (+/-): 16/22\n",
      "---\n",
      "For Diagnosis AD (N=22)\n",
      "Sex (f/m): 10/12\n",
      "Age: 75.44 ± 8.51\n",
      "Global Tau: 1.36 ± 0.35\n",
      "Global Amyloid: 1.51 ± 0.34\n",
      "Amyloid Status (+/-): 18/4\n",
      "Tau Status (+/-): 11/11\n",
      "MMSE: 24.33 ± 3.51\n",
      "ADAS13: 21.33 ± 7.53\n",
      "APOE Status (+/-): 7/4\n",
      "---\n",
      "@@@@\n"
     ]
    }
   ],
   "source": [
    "# Split by diagnosis and calculate mean age, sex, amyloid SUVR and tau SUVR\n",
    "print(f\"Train AV45 N={len(tau_abeta_AV45_overlap_suvr_df_diag)}\")\n",
    "print(f\"Test FBB N={len(tau_abeta_FBB_overlap_suvr_df_diag)}\")\n",
    "print('---')\n",
    "for split_df in [tau_abeta_AV45_overlap_suvr_df_diag, tau_abeta_FBB_overlap_suvr_df_diag]:\n",
    "  for d in ['CN','MCI','AD']:\n",
    "    temp_diag_df = split_df[split_df['diag']==d]\n",
    "    f_count = sum(temp_diag_df['Sex']=='F')\n",
    "    m_count = sum(temp_diag_df['Sex']=='M')\n",
    "    mean_age = temp_diag_df['Age'].mean()\n",
    "    std_age = temp_diag_df['Age'].std()\n",
    "    mean_global_tau = temp_diag_df['tau.global.SUVR_tau'].mean()\n",
    "    std_global_tau = temp_diag_df['tau.global.SUVR_tau'].std()\n",
    "    mean_global_amyloid = temp_diag_df['amyloid.global.SUVR_abeta'].mean()\n",
    "    std_global_amyloid = temp_diag_df['amyloid.global.SUVR_abeta'].std()\n",
    "    amyloid_pos_count = sum(temp_diag_df['A+']==1)\n",
    "    amyloid_neg_count = sum(temp_diag_df['A+']==0)\n",
    "    tau_pos_count = sum(temp_diag_df['T+']==1)\n",
    "    tau_neg_count = sum(temp_diag_df['T+']==0)\n",
    "    mean_mmse = temp_diag_df['MMSE_tau'].mean()\n",
    "    std_mmse = temp_diag_df['MMSE_tau'].std()\n",
    "    mean_adas13 = temp_diag_df['ADAS13_tau'].mean()\n",
    "    std_adas13 = temp_diag_df['ADAS13_tau'].std()\n",
    "    apoe_neg_count = sum(temp_diag_df['apoe_tau']==33)\n",
    "    apoe_pos_count = sum(temp_diag_df['apoe_tau']==34)+sum(temp_diag_df['apoe_tau']==43)\n",
    "    print(f\"For Diagnosis {d} (N={len(temp_diag_df)})\\nSex (f/m): {f_count}/{m_count}\\nAge: {mean_age:.2f} ± {std_age:.2f}\\nGlobal Tau: {mean_global_tau:.2f} ± {std_global_tau:.2f}\\nGlobal Amyloid: {mean_global_amyloid:.2f} ± {std_global_amyloid:.2f}\\nAmyloid Status (+/-): {amyloid_pos_count}/{amyloid_neg_count}\\nTau Status (+/-): {tau_pos_count}/{tau_neg_count}\\nMMSE: {mean_mmse:.2f} ± {std_mmse:.2f}\\nADAS13: {mean_adas13:.2f} ± {std_adas13:.2f}\\nAPOE Status (+/-): {apoe_pos_count}/{apoe_neg_count}\")\n",
    "    print('---')\n",
    "  print('@@@@')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "105\n",
      "105\n",
      "105\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "all_groups = split_groups(tau_abeta_AV45_overlap_suvr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_apoe_one_hot(apoe):\n",
    "    if apoe == '33':\n",
    "        return '1,0,0,0,0,0'\n",
    "    if apoe == '34':\n",
    "        return '0,1,0,0,0,0'\n",
    "    if apoe == '44':\n",
    "        return '0,0,1,0,0,0'\n",
    "    if apoe == '24':\n",
    "        return '0,0,0,1,0,0'\n",
    "    if apoe == '23':\n",
    "        return '0,0,0,0,1,0'\n",
    "    if apoe == '22':\n",
    "        return '0,0,0,0,0,1'\n",
    "    \n",
    "def encode_suvr(row,protein):\n",
    "    out = ''\n",
    "    for i in dktLabels:\n",
    "        out += '{},'.format(row['SUVR.DKT.ROI.idx.{}_{}'.format(i,protein)])\n",
    "    return out[:-1]\n",
    "\n",
    "def encode_suvr_fbb_as_av45(row,protein):\n",
    "    out = ''\n",
    "    for i in dktLabels:\n",
    "        out += '{},'.format(Calc_AV45_From_FBB(row['SUVR.DKT.ROI.idx.{}_{}'.format(i,protein)]))\n",
    "    return out[:-1]\n",
    "\n",
    "\n",
    "def encode_m_f(sex):\n",
    "    if sex == 'M':\n",
    "        return '0,1'\n",
    "    else:\n",
    "        return '1,0'\n",
    "\n",
    "mean_age = suvr_df[suvr_df['pet.modality']=='pet-AV45']['age'].mean()\n",
    "\n",
    "if SAVE:\n",
    "    with open(os.path.join(PROJECT_BASE,'data/splits_FBB_as_AV45_multitask/dkt/split_5_diag.csv'), 'w') as f:\n",
    "        for indx, row in tau_abeta_FBB_overlap_suvr_df_diag.iterrows():\n",
    "            out = '{},{},{},{},{},{},{}\\n'.format(encode_m_f(row['Sex']),str(row['Age']-mean_age),encode_apoe_one_hot(row['apoe']),encode_suvr(row,'tau'),encode_suvr_fbb_as_av45(row,'abeta'),row['A+'],row['diag'])\n",
    "            f.write(out)\n",
    "      \n",
    "    with open(os.path.join(PROJECT_BASE,'data/splits_FBB_as_AV45_multitask/dkt/split_5.csv'), 'w') as f:\n",
    "        for indx, row in tau_abeta_FBB_overlap_suvr_df_diag.iterrows():\n",
    "            out = '{},{},{},{},{},{}\\n'.format(encode_m_f(row['Sex']),str(row['Age']-mean_age),encode_apoe_one_hot(row['apoe']),encode_suvr(row,'tau'),encode_suvr_fbb_as_av45(row,'abeta'),row['A+'])\n",
    "            f.write(out)\n",
    "\n",
    "    for s,g in enumerate(all_groups):\n",
    "        with open(os.path.join(PROJECT_BASE,'data/splits_AV45_multitask/dkt/split_{}.csv'.format(s)), 'w') as f:\n",
    "            for i in g:\n",
    "                row = tau_abeta_AV45_overlap_suvr_df.loc[i]\n",
    "                #out = '{},{},{},{},{}\\n'.format(encode_m_f(row['Sex']),str(row['Age']-mean_age),encode_apoe_one_hot(row['apoe']),encode_suvr(row,'tau'),encode_suvr_fbb_as_av45(row,'abeta'))\n",
    "                out = '{},{},{},{},{},{}\\n'.format(encode_m_f(row['Sex']),str(row['Age']-mean_age),encode_apoe_one_hot(row['apoe']),encode_suvr(row,'tau'),encode_suvr(row,'abeta'),row['A+'])\n",
    "                f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PROJECT_BASE,'data/splits_FBB_as_AV45_multitask/dkt/split_5_diags.csv'), 'w') as f:\n",
    "        for indx, row in tau_abeta_FBB_overlap_suvr_df_diag.iterrows():\n",
    "            out = '{}\\n'.format(row['diag'])\n",
    "            f.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create corresponding gmvolume dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which volume files are on PC\n",
    "t1_tau_overlap_df['RID'] = t1_tau_overlap_df.apply(lambda row: int(row['Subject ID'].split('_')[2]), axis=1)\n",
    "\n",
    "my_files_df = pd.DataFrame(columns=['RID','Study Date'])\n",
    "for filename in glob.iglob('{}/**/*AALVolume_GM_visit_masked.csv'.format(ADNI_DATA_BASE),recursive = True):\n",
    "    split = filename.split('/')\n",
    "    rid = split[-5].split('sub-')[1]\n",
    "    ses = split[-3].split('ses-')[1]\n",
    "    my_files_df = my_files_df.append({'RID':rid,'Study Date':ses}, ignore_index=True)\n",
    "my_files_df['Study Date'] = pd.to_datetime(my_files_df['Study Date'])\n",
    "my_files_df['RID'] = pd.to_numeric(my_files_df['RID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6211"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_files_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 215 Amyloid/Tau files, 208 have an MRI within 6 months of the Tau-PET\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N+</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N+  count\n",
       "0   0    167\n",
       "1   1     41"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neurodegen Test\n",
    "available_t1_tau_df = t1_tau_overlap_df.merge(my_files_df, right_on=['RID','Study Date'], left_on=['RID','Study Date_t1'])\n",
    "t1_tau_FBB_df = available_t1_tau_df.merge(tau_abeta_FBB_overlap_suvr_df, on=['RID','Study Date_tau'])\n",
    "print(\"Out of {} Amyloid/Tau files, {} have an MRI within 6 months of the Tau-PET\".format(len(tau_abeta_FBB_overlap_suvr_df),len(t1_tau_FBB_df)))\n",
    "out_split_df_test = t1_tau_FBB_df\n",
    "\n",
    "# Grab SUVR values\n",
    "suvr_df = pd.read_csv(os.path.join(DATA_BASE,'ADNI_Tau_Amyloid_SUVR_status.csv'))\n",
    "suvr_df['RID'] = pd.to_numeric(suvr_df['RID'])\n",
    "suvr_df['acq.date'] = pd.to_datetime(suvr_df['acq.date'])\n",
    "t1_tau_overlap_df_copy = out_split_df_test.copy(deep=True)\n",
    "t1_tau_overlap_suvr_df = t1_tau_overlap_df_copy.merge(suvr_df[suvr_df['pet.modality']=='pet-AV1451'], how='left', left_on=['RID','Study Date_tau'], right_on=['RID','acq.date'])\n",
    "t1_tau_overlap_suvr_df = t1_tau_overlap_suvr_df.dropna(subset=['SUVR.Schaefer200.ROI.idx.1','apoe_y'])\n",
    "\n",
    "len(t1_tau_overlap_suvr_df)\n",
    "out_df_test=t1_tau_overlap_suvr_df.rename(columns={'apoe_y':'apoe'})\n",
    "\n",
    "# Grab gmvol values\n",
    "GM_volume_df_test = pd.DataFrame(columns=['RID','Study Date'].extend(commonLabels))\n",
    "for indx, row in out_df_test.iterrows():\n",
    "    rid = row['RID']\n",
    "    date = row['Study Date_t1']\n",
    "    filename = os.path.join(ADNI_DATA_BASE,'sub-{}'.format(str(rid).zfill(4)),\"anat\",\"ses-{}\".format(str(date)[:10]),\"antsCorticalThickness\",'AALVolume_GM_visit_masked.csv')\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    means = [float(l.split()[1]) for i,l in enumerate(lines) if i>0]\n",
    "    labels = [int(l.split()[0]) for i,l in enumerate(lines) if i>0]\n",
    "    toKeep = []\n",
    "    for i,l in enumerate(labels):\n",
    "        if l in commonLabels:\n",
    "            toKeep.append(i)\n",
    "    if len(toKeep) != len(commonLabels):\n",
    "        print('skip | {} | {} | {}'.format(len(toKeep),len(commonLabels),filename))\n",
    "        continue\n",
    "    means = [means[i] for i in toKeep]\n",
    "    newRow = {'RID':int(rid),'Study Date':date}\n",
    "    for i, l in enumerate(commonLabels):\n",
    "        newRow[l] = means[i]\n",
    "    GM_volume_df_test = GM_volume_df_test.append(newRow, ignore_index=True)\n",
    "\n",
    "# merge gmvol values and suvr values\n",
    "out_GM_volume_df_test = out_df_test.merge(GM_volume_df_test, left_on=['RID','Study Date_t1'], right_on=['RID','Study Date'])\n",
    "\n",
    "# Calc N status\n",
    "out_GM_volume_df_test['Hippocampus Volume'] = out_GM_volume_df_test.apply(lambda row: calc_hipp_vol(row),axis=1)\n",
    "cutoff = 1.6474876557377052 - 1 * 0.08779840023070665 # Hipp only\n",
    "cutoff = 1.6617301245901637 - 1 * 0.08045998796511386\n",
    "out_GM_volume_df_test['N+'] = out_GM_volume_df_test.apply(lambda row: 1 if row['Hippocampus Volume']<cutoff else 0, axis=1)\n",
    "\n",
    "out_GM_volume_df_test.groupby([\"N+\"]).size().reset_index().rename(columns={0:'count'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1154 Tau files, 946 have an MRI within 6 months of the Tau-PET and are not FBB\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4400/anat/ses-2017-09-26/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 97 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4197/anat/ses-2015-09-21/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N+</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N+  count\n",
       "0   0    499\n",
       "1   1    200"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neurodegen train\n",
    "merge = available_t1_tau_df.merge(tau_abeta_FBB_overlap_suvr_df, on=['RID','Study Date_tau'], how='outer', indicator='source')\n",
    "t1_tau_noFBB_df = merge[merge.source.eq('left_only')].drop('source', axis=1)\n",
    "\n",
    "print(\"Out of {} Tau files, {} have an MRI within 6 months of the Tau-PET and are not FBB\".format(len(available_t1_tau_df),len(t1_tau_noFBB_df)))\n",
    "out_split_df_train = t1_tau_noFBB_df\n",
    "\n",
    "# Grab SUVR values\n",
    "suvr_df = pd.read_csv(os.path.join(DATA_BASE,'ADNI_Tau_Amyloid_SUVR_status.csv'))\n",
    "suvr_df['RID'] = pd.to_numeric(suvr_df['RID'])\n",
    "suvr_df['acq.date'] = pd.to_datetime(suvr_df['acq.date'])\n",
    "t1_tau_overlap_df_copy = out_split_df_train.copy(deep=True)\n",
    "t1_tau_overlap_suvr_df = t1_tau_overlap_df_copy.merge(suvr_df[suvr_df['pet.modality']=='pet-AV1451'], how='left', left_on=['RID','Study Date_tau'], right_on=['RID','acq.date'])\n",
    "t1_tau_overlap_suvr_df = t1_tau_overlap_suvr_df.dropna(subset=['SUVR.Schaefer200.ROI.idx.1','apoe_y'])\n",
    "\n",
    "len(t1_tau_overlap_suvr_df)\n",
    "out_df_train=t1_tau_overlap_suvr_df.rename(columns={'apoe_y':'apoe'})\n",
    "\n",
    "# Grab gmvol values\n",
    "GM_volume_df_train = pd.DataFrame(columns=['RID','Study Date'].extend(commonLabels))\n",
    "for indx, row in out_df_train.iterrows():\n",
    "    rid = row['RID']\n",
    "    date = row['Study Date_t1']\n",
    "    filename = os.path.join(ADNI_DATA_BASE,'sub-{}'.format(str(rid).zfill(4)),\"anat\",\"ses-{}\".format(str(date)[:10]),\"antsCorticalThickness\",'AALVolume_GM_visit_masked.csv')\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    means = [float(l.split()[1]) for i,l in enumerate(lines) if i>0]\n",
    "    labels = [int(l.split()[0]) for i,l in enumerate(lines) if i>0]\n",
    "    toKeep = []\n",
    "    for i,l in enumerate(labels):\n",
    "        if l in commonLabels:\n",
    "            toKeep.append(i)\n",
    "    if len(toKeep) != len(commonLabels):\n",
    "        print('skip | {} | {} | {}'.format(len(toKeep),len(commonLabels),filename))\n",
    "        continue\n",
    "    means = [means[i] for i in toKeep]\n",
    "    newRow = {'RID':int(rid),'Study Date':date}\n",
    "    for i, l in enumerate(commonLabels):\n",
    "        newRow[l] = means[i]\n",
    "    GM_volume_df_train = GM_volume_df_train.append(newRow, ignore_index=True)\n",
    "    \n",
    "# merge gmvol values and suvr values\n",
    "out_GM_volume_df_train = out_df_train.merge(GM_volume_df_train, left_on=['RID','Study Date_t1'], right_on=['RID','Study Date'])\n",
    "\n",
    "# Calc N status\n",
    "out_GM_volume_df_train['Hippocampus Volume'] = out_GM_volume_df_train.apply(lambda row: calc_hipp_vol(row),axis=1)\n",
    "cutoff = 1.6474876557377052 - 1 * 0.08779840023070665 # Hipp only\n",
    "cutoff = 1.6617301245901637 - 1 * 0.08045998796511386\n",
    "out_GM_volume_df_train['N+'] = out_GM_volume_df_train.apply(lambda row: 1 if row['Hippocampus Volume']<cutoff else 0, axis=1)\n",
    "\n",
    "out_GM_volume_df_train.groupby([\"N+\"]).size().reset_index().rename(columns={0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "print(len(out_GM_volume_df_train))\n",
    "print(len(out_GM_volume_df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calc Population Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_GM_volume_df_train_diag = get_overlap_dataframe(out_GM_volume_df_train, diag_df, df1_id_name='Subject ID_x', df1_date_name='Study Date_tau')\n",
    "out_GM_volume_df_train_diag['diag'] = out_GM_volume_df_train_diag.apply(lambda x: get_diag(x),axis=1)\n",
    "out_GM_volume_df_train_diag['T+'] = out_GM_volume_df_train_diag.apply(lambda row: 1 if row['tau.global.SUVR']>1.3 else 0, axis=1)\n",
    "\n",
    "out_GM_volume_df_test_diag = get_overlap_dataframe(out_GM_volume_df_test, diag_df, df1_id_name='Subject ID_x', df1_date_name='Study Date_tau', days_delta=600)\n",
    "out_GM_volume_df_test_diag['diag'] = out_GM_volume_df_test_diag.apply(lambda x: get_diag(x),axis=1)\n",
    "out_GM_volume_df_test_diag['T+'] = out_GM_volume_df_test_diag.apply(lambda row: 1 if row['tau.global.SUVR']>1.3 else 0, axis=1)\n",
    "\n",
    "all_subjects_df = pd.concat([out_GM_volume_df_train,out_GM_volume_df_test])\n",
    "all_subjects_diag_df = get_overlap_dataframe(all_subjects_df, diag_df, df1_id_name='Subject ID_x', df1_date_name='Study Date_tau')\n",
    "all_subjects_diag_df['diag'] = all_subjects_diag_df.apply(lambda x: get_diag(x),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "207\n"
     ]
    }
   ],
   "source": [
    "print(len(out_GM_volume_df_test))\n",
    "print(len(out_GM_volume_df_test_diag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train N=671\n",
      "Test N=207\n",
      "---\n",
      "For Diagnosis CN (N=404)\n",
      "Sex (f/m): 236/168\n",
      "Age: 74.98 ± 6.96\n",
      "Global Tau: 1.08 ± 0.09\n",
      "Neurodegen Status (+/-): 64/340\n",
      "Tau Status (+/-): 8/396\n",
      "MMSE: 29.09 ± 1.17\n",
      "ADAS13: 7.63 ± 4.43\n",
      "APOE Status (+/-): 131/215\n",
      "---\n",
      "@@@@\n",
      "For Diagnosis MCI (N=201)\n",
      "Sex (f/m): 78/123\n",
      "Age: 76.95 ± 7.06\n",
      "Global Tau: 1.13 ± 0.16\n",
      "Neurodegen Status (+/-): 48/153\n",
      "Tau Status (+/-): 18/183\n",
      "MMSE: 27.93 ± 2.00\n",
      "ADAS13: 13.32 ± 6.04\n",
      "APOE Status (+/-): 56/105\n",
      "---\n",
      "@@@@\n",
      "For Diagnosis AD (N=66)\n",
      "Sex (f/m): 27/39\n",
      "Age: 76.97 ± 9.29\n",
      "Global Tau: 1.35 ± 0.42\n",
      "Neurodegen Status (+/-): 36/30\n",
      "Tau Status (+/-): 25/41\n",
      "MMSE: 21.71 ± 4.85\n",
      "ADAS13: 29.93 ± 10.40\n",
      "APOE Status (+/-): 21/29\n",
      "---\n",
      "@@@@\n",
      "For Diagnosis CN (N=139)\n",
      "Sex (f/m): 87/52\n",
      "Age: 71.05 ± 5.94\n",
      "Global Tau: 1.08 ± 0.11\n",
      "Neurodegen Status (+/-): 14/125\n",
      "Tau Status (+/-): 5/134\n",
      "MMSE: 29.54 ± 0.78\n",
      "ADAS13: 8.44 ± 4.89\n",
      "APOE Status (+/-): 50/74\n",
      "---\n",
      "@@@@\n",
      "For Diagnosis MCI (N=46)\n",
      "Sex (f/m): 20/26\n",
      "Age: 71.14 ± 8.05\n",
      "Global Tau: 1.13 ± 0.19\n",
      "Neurodegen Status (+/-): 3/43\n",
      "Tau Status (+/-): 5/41\n",
      "MMSE: 27.50 ± 1.91\n",
      "ADAS13: 12.50 ± 0.84\n",
      "APOE Status (+/-): 16/22\n",
      "---\n",
      "@@@@\n",
      "For Diagnosis AD (N=22)\n",
      "Sex (f/m): 10/12\n",
      "Age: 75.37 ± 8.51\n",
      "Global Tau: 1.36 ± 0.35\n",
      "Neurodegen Status (+/-): 13/9\n",
      "Tau Status (+/-): 11/11\n",
      "MMSE: 24.33 ± 3.51\n",
      "ADAS13: 21.33 ± 7.53\n",
      "APOE Status (+/-): 7/4\n",
      "---\n",
      "@@@@\n"
     ]
    }
   ],
   "source": [
    "# Split by diagnosis and calculate mean age, sex, amyloid SUVR and tau SUVR\n",
    "print(f\"Train N={len(out_GM_volume_df_train_diag)}\")\n",
    "print(f\"Test N={len(out_GM_volume_df_test_diag)}\")\n",
    "print('---')\n",
    "for split_df in [out_GM_volume_df_train_diag, out_GM_volume_df_test_diag]:\n",
    "    for d in ['CN','MCI','AD']:\n",
    "        temp_diag_df = split_df[split_df['diag']==d]\n",
    "        f_count = sum(temp_diag_df['Sex_x']=='F')\n",
    "        m_count = sum(temp_diag_df['Sex_x']=='M')\n",
    "        mean_age = temp_diag_df['Age_x'].mean()\n",
    "        std_age = temp_diag_df['Age_x'].std()\n",
    "        mean_global_tau = temp_diag_df['tau.global.SUVR'].mean()\n",
    "        std_global_tau = temp_diag_df['tau.global.SUVR'].std()\n",
    "        tau_pos_count = sum(temp_diag_df['T+']==1)\n",
    "        tau_neg_count = sum(temp_diag_df['T+']==0)\n",
    "        neurodegen_pos_count = sum(temp_diag_df['N+']==1)\n",
    "        neurodegen_neg_count = sum(temp_diag_df['N+']==0)\n",
    "        mean_mmse = temp_diag_df['MMSE'].mean()\n",
    "        std_mmse = temp_diag_df['MMSE'].std()\n",
    "        mean_adas13 = temp_diag_df['ADAS13'].mean()\n",
    "        std_adas13 = temp_diag_df['ADAS13'].std()\n",
    "        apoe_neg_count = sum(temp_diag_df['apoe']==33)\n",
    "        apoe_pos_count = sum(temp_diag_df['apoe']==34)+sum(temp_diag_df['apoe']==43)\n",
    "        print(f\"For Diagnosis {d} (N={len(temp_diag_df)})\\nSex (f/m): {f_count}/{m_count}\\nAge: {mean_age:.2f} ± {std_age:.2f}\\nGlobal Tau: {mean_global_tau:.2f} ± {std_global_tau:.2f}\\nNeurodegen Status (+/-): {neurodegen_pos_count}/{neurodegen_neg_count}\\nTau Status (+/-): {tau_pos_count}/{tau_neg_count}\\nMMSE: {mean_mmse:.2f} ± {std_mmse:.2f}\\nADAS13: {mean_adas13:.2f} ± {std_adas13:.2f}\\nAPOE Status (+/-): {apoe_pos_count}/{apoe_neg_count}\")\n",
    "        print('---')\n",
    "        print('@@@@')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "140\n",
      "140\n",
      "140\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "all_groups = split_groups(out_GM_volume_df_train, num_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_GM_volume_df_test = out_GM_volume_df_test.drop_duplicates(subset=['RID','Study Date_tau'])\n",
    "test_ids_gmdensity = []\n",
    "for indx, row in out_GM_volume_df_test.iterrows():\n",
    "    test_ids_gmdensity.append('{}|{}'.format(row['RID'],row['Study Date_tau']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_apoe_one_hot(apoe):\n",
    "    apoe = str(int(apoe))\n",
    "    if apoe == '33':\n",
    "        return '1,0,0,0,0,0'\n",
    "    if apoe == '34':\n",
    "        return '0,1,0,0,0,0'\n",
    "    if apoe == '44':\n",
    "        return '0,0,1,0,0,0'\n",
    "    if apoe == '24':\n",
    "        return '0,0,0,1,0,0'\n",
    "    if apoe == '23':\n",
    "        return '0,0,0,0,1,0'\n",
    "    if apoe == '22':\n",
    "        return '0,0,0,0,0,1'\n",
    "    \n",
    "def encode_suvr_dkt(row,protein):\n",
    "    out = ''\n",
    "    for i in dktLabels:\n",
    "        out += '{},'.format(row['SUVR.DKT.ROI.idx.{}'.format(i,protein)])\n",
    "    return out[:-1]\n",
    "\n",
    "def encode_suvr_schaefer(row,protein):\n",
    "    out = ''\n",
    "    for i in range(1,201):\n",
    "        out += '{},'.format(row['SUVR.Schaefer200.ROI.idx.{}'.format(i)])\n",
    "    return out[:-1]\n",
    "\n",
    "def encode_vol(row):\n",
    "    out = ''\n",
    "    for l in commonLabels:\n",
    "        out += '{},'.format(row[l])\n",
    "    return out[:-1]\n",
    "\n",
    "def encode_m_f(sex):\n",
    "    if sex == 'M':\n",
    "        return '0,1'\n",
    "    else:\n",
    "        return '1,0'\n",
    "\n",
    "mean_age = out_GM_volume_df_train['Age_x'].mean()\n",
    "\n",
    "folder = 'splits_volume_ATN_schaefer_multitask'\n",
    "enc = encode_suvr_schaefer\n",
    "cnt = 0\n",
    "\n",
    "if SAVE:\n",
    "    for s,g in enumerate(all_groups):\n",
    "        with open(os.path.join(PROJECT_BASE,'data/{}/split_{}.csv'.format(folder,s)), 'w') as f:\n",
    "            for i in g:\n",
    "                row = out_GM_volume_df_train.loc[i]\n",
    "                out = '{},{},{},{},{},{}\\n'.format(encode_m_f(row['Sex_x']),str(row['Age_x']-mean_age),encode_apoe_one_hot(row['apoe']),enc(row,'tau'),encode_vol(row),row['N+'])\n",
    "                if 'na' not in out:\n",
    "                    f.write(out)\n",
    "                else:\n",
    "                    cnt = cnt+1\n",
    "                    print(cnt)\n",
    "    s=5\n",
    "    with open(os.path.join(PROJECT_BASE,'data/{}/split_{}.csv'.format(folder,s)), 'w') as f:\n",
    "        for indx, row in out_GM_volume_df_test.iterrows():\n",
    "            row = out_GM_volume_df_test.loc[indx]\n",
    "            out = '{},{},{},{},{},{}\\n'.format(encode_m_f(row['Sex_x']),str(row['Age_x']-mean_age),encode_apoe_one_hot(row['apoe']),enc(row,'tau'),encode_vol(row),row['N+'])\n",
    "            f.write(out)\n",
    "\n",
    "    with open(os.path.join(PROJECT_BASE,'data/{}/split_{}_diag.csv'.format(folder,s)), 'w') as f:\n",
    "        for indx, row in out_GM_volume_df_test_diag.iterrows():\n",
    "            row = out_GM_volume_df_test_diag.loc[indx]\n",
    "            out = '{},{},{},{},{},{},{}\\n'.format(encode_m_f(row['Sex_x']),str(row['Age_x']-mean_age),encode_apoe_one_hot(row['apoe']),enc(row,'tau'),encode_vol(row),row['N+'],row['diag'])\n",
    "            f.write(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_apoe_one_hot(apoe):\n",
    "    apoe = str(int(apoe))\n",
    "    if apoe == '33':\n",
    "        return '1,0,0,0,0,0'\n",
    "    if apoe == '34':\n",
    "        return '0,1,0,0,0,0'\n",
    "    if apoe == '44':\n",
    "        return '0,0,1,0,0,0'\n",
    "    if apoe == '24':\n",
    "        return '0,0,0,1,0,0'\n",
    "    if apoe == '23':\n",
    "        return '0,0,0,0,1,0'\n",
    "    if apoe == '22':\n",
    "        return '0,0,0,0,0,1'\n",
    "    \n",
    "def encode_suvr_dkt(row,protein):\n",
    "    out = ''\n",
    "    for i in dktLabels:\n",
    "        out += '{},'.format(row['SUVR.DKT.ROI.idx.{}'.format(i,protein)])\n",
    "    return out[:-1]\n",
    "\n",
    "def encode_suvr_schaefer(row,protein):\n",
    "    out = ''\n",
    "    for i in range(1,201):\n",
    "        out += '{},'.format(row['SUVR.Schaefer200.ROI.idx.{}'.format(i)])\n",
    "    return out[:-1]\n",
    "\n",
    "def encode_vol(row):\n",
    "    out = ''\n",
    "    for l in commonLabels:\n",
    "        out += '{},'.format(row[l])\n",
    "    return out[:-1]\n",
    "\n",
    "def encode_m_f(sex):\n",
    "    if sex == 'M':\n",
    "        return '0,1'\n",
    "    else:\n",
    "        return '1,0'\n",
    "\n",
    "mean_age = out_GM_volume_df_train['Age_x'].mean()\n",
    "\n",
    "folder = 'splits_volume_ATN_schaefer_multitask'\n",
    "enc = encode_suvr_schaefer\n",
    "cnt = 0\n",
    "s=5\n",
    "with open(os.path.join(PROJECT_BASE,'data/{}/split_{}_diag.csv'.format(folder,s)), 'w') as f:\n",
    "        for indx, row in out_GM_volume_df_test_diag.iterrows():\n",
    "            row = out_GM_volume_df_test_diag.loc[indx]\n",
    "            out = '{},{},{},{},{},{},{}\\n'.format(encode_m_f(row['Sex_x']),str(row['Age_x']-mean_age),encode_apoe_one_hot(row['apoe']),enc(row,'tau'),encode_vol(row),row['N+'],row['diag'])\n",
    "            f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PROJECT_BASE,'data/{}/split_{}_diags.csv'.format(folder,s)), 'w') as f:\n",
    "  for indx, row in out_GM_volume_df_test_diag.iterrows():\n",
    "    f.write('{}\\n'.format(row['diag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PROJECT_BASE,'data/splits_volume_ATN_schaefer_multitask/split_5_ids.csv'), 'w') as f:\n",
    "        out = 'RID,Study Date Tau,Study Date T1,Diagnosis\\n'\n",
    "        f.write(out)\n",
    "        for indx, row in out_GM_volume_df_test_diag.iterrows():\n",
    "            out = f'{row[\"RID\"]},{row[\"Study Date_tau\"]},{row[\"Study Date_t1\"]},{row[\"diag\"]}\\n'\n",
    "            f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amyloid test set size is 215, and gm density is 207\n",
      "207\n",
      "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,54,55,56,57,58,59,60,61,62,63,64,65,66,67,69,70,71,72,73,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,212,213,214\n"
     ]
    }
   ],
   "source": [
    "# Check overlap of test sets\n",
    "print('Amyloid test set size is {}, and gm density is {}'.format(len(test_ids_amyloid),len(test_ids_gmdensity)))\n",
    "test_ids_amyloid_overlap_gmdensity_indices_str = [str(i) for i in range(len(test_ids_amyloid)) if test_ids_amyloid[i] in test_ids_gmdensity]\n",
    "test_ids_amyloid_overlap_gmdensity_indices = [i for i in range(len(test_ids_amyloid)) if test_ids_amyloid[i] in test_ids_gmdensity]\n",
    "print(len(test_ids_amyloid_overlap_gmdensity_indices))\n",
    "print(','.join(test_ids_amyloid_overlap_gmdensity_indices_str))\n",
    "if False:\n",
    "    for i in range(len(test_ids_amyloid_overlap_gmdensity_indices)):\n",
    "        #print(test_ids_amyloid_overlap_gmdensity_indices[i])\n",
    "        print('Amyloid: {} | GMDensity: {}'.format(test_ids_amyloid[test_ids_amyloid_overlap_gmdensity_indices[i]],test_ids_gmdensity[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc mean hippocampus volume A-/T-/CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4400/anat/ses-2017-09-26/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-6297/anat/ses-2019-05-28/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 89 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4827/anat/ses-2013-01-30/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-6519/anat/ses-2018-07-31/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-6519/anat/ses-2020-10-01/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2392/anat/ses-2012-08-17/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 97 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4197/anat/ses-2015-09-21/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4536/anat/ses-2014-03-25/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 72 | 98 | /home/guests/projects/adni/data_bids_processed/sub-6597/anat/ses-2018-11-21/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-6597/anat/ses-2020-08-25/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 61 | 98 | /home/guests/projects/adni/data_bids_processed/sub-0413/anat/ses-2007-06-01/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-0023/anat/ses-2013-12-23/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-0096/anat/ses-2012-02-03/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-0830/anat/ses-2006-09-21/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-0830/anat/ses-2007-09-17/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-0886/anat/ses-2007-04-11/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2027/anat/ses-2010-07-22/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2060/anat/ses-2015-10-30/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2073/anat/ses-2014-09-17/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2093/anat/ses-2011-04-15/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2109/anat/ses-2011-05-19/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2148/anat/ses-2010-10-26/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2148/anat/ses-2011-02-16/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2148/anat/ses-2011-05-26/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2148/anat/ses-2011-11-23/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2148/anat/ses-2013-01-29/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2148/anat/ses-2013-12-17/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 9 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2153/anat/ses-2012-02-22/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2168/anat/ses-2013-12-20/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2171/anat/ses-2011-06-21/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2195/anat/ses-2010-11-19/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2195/anat/ses-2011-02-21/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2195/anat/ses-2011-06-13/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2195/anat/ses-2011-12-13/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2195/anat/ses-2012-12-05/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2195/anat/ses-2013-12-05/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2195/anat/ses-2014-12-16/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-2307/anat/ses-2012-04-03/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4034/anat/ses-2011-06-07/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4034/anat/ses-2011-08-26/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4034/anat/ses-2011-11-16/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4034/anat/ses-2012-07-26/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4041/anat/ses-2012-06-18/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4133/anat/ses-2013-08-08/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 93 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4196/anat/ses-2011-09-13/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 97 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4196/anat/ses-2012-01-27/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4203/anat/ses-2013-09-23/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4205/anat/ses-2012-11-01/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4225/anat/ses-2011-09-21/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4225/anat/ses-2011-12-26/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4225/anat/ses-2012-04-12/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4225/anat/ses-2012-10-11/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4225/anat/ses-2013-10-17/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4225/anat/ses-2018-11-12/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4241/anat/ses-2012-09-27/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4339/anat/ses-2012-02-16/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4339/anat/ses-2012-05-30/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4420/anat/ses-2013-03-25/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4466/anat/ses-2013-03-01/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4502/anat/ses-2012-03-06/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4502/anat/ses-2012-06-25/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4502/anat/ses-2012-10-25/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4502/anat/ses-2013-03-28/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4502/anat/ses-2014-04-23/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4502/anat/ses-2016-04-12/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4522/anat/ses-2014-03-04/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4595/anat/ses-2012-10-24/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4610/anat/ses-2013-04-09/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4733/anat/ses-2012-11-30/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4737/anat/ses-2012-06-08/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4737/anat/ses-2012-08-28/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4780/anat/ses-2014-07-24/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4823/anat/ses-2015-07-06/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4853/anat/ses-2012-07-11/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4853/anat/ses-2012-10-23/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4853/anat/ses-2013-08-30/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4867/anat/ses-2013-03-04/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4889/anat/ses-2012-11-14/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4905/anat/ses-2012-08-16/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4905/anat/ses-2012-11-16/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4905/anat/ses-2013-03-18/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4905/anat/ses-2013-08-26/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4982/anat/ses-2012-10-23/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4982/anat/ses-2013-01-25/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4982/anat/ses-2013-04-26/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-4982/anat/ses-2013-10-25/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-5006/anat/ses-2013-05-17/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-5047/anat/ses-2012-12-07/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-5121/anat/ses-2015-04-30/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-5131/anat/ses-2015-04-16/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-5138/anat/ses-2013-04-02/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-5138/anat/ses-2013-07-02/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-5202/anat/ses-2015-09-21/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-5285/anat/ses-2013-10-28/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-5285/anat/ses-2016-05-03/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-5285/anat/ses-2018-10-02/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-5285/anat/ses-2019-06-28/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n",
      "skip | 0 | 98 | /home/guests/projects/adni/data_bids_processed/sub-6686/anat/ses-2019-02-20/antsCorticalThickness/AALVolume_GM_visit_masked.csv\n"
     ]
    }
   ],
   "source": [
    "GM_volume_df = pd.DataFrame(columns=['RID','Study Date'].extend(commonLabels))\n",
    "for indx, row in my_files_df.iterrows():\n",
    "    rid = row['RID']\n",
    "    date = row['Study Date']\n",
    "    filename = os.path.join(ADNI_DATA_BASE,'sub-{}'.format(str(rid).zfill(4)),\"anat\",\"ses-{}\".format(str(date)[:10]),\"antsCorticalThickness\",'AALVolume_GM_visit_masked.csv')\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    means = [float(l.split()[1]) for i,l in enumerate(lines) if i>0]\n",
    "    labels = [int(l.split()[0]) for i,l in enumerate(lines) if i>0]\n",
    "    toKeep = []\n",
    "    for i,l in enumerate(labels):\n",
    "        if l in commonLabels:\n",
    "            toKeep.append(i)\n",
    "    if len(toKeep) != len(commonLabels):\n",
    "        print('skip | {} | {} | {}'.format(len(toKeep),len(commonLabels),filename))\n",
    "        continue\n",
    "    means = [means[i] for i in toKeep]\n",
    "    newRow = {'RID':int(rid),'Study Date':date}\n",
    "    for i, l in enumerate(commonLabels):\n",
    "        newRow[l] = means[i]\n",
    "    GM_volume_df = GM_volume_df.append(newRow, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM_density_tau_amyloid_df = get_overlap_dataframe(GM_volume_df,tau_abeta_overlap_suvr_df,df2_name='diag',df1_name='vol',df1_id_name='RID',df2_id_name='RID',df2_date_name='Study Date_tau')\n",
    "GM_density_tau_amyloid_suvr_diag_df = get_overlap_dataframe(GM_density_tau_amyloid_df,diag_df,df2_name='diag',df1_name='vol',df1_id_name='RID',df2_id_name='RID',df1_date_name='Study Date_tau')\n",
    "GM_density_tau_amyloid_suvr_diag_df['Diag'] = GM_density_tau_amyloid_suvr_diag_df.apply(lambda row: get_diag(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of A-/T- subjects, the mean total GM volume was 1.6617301245901637 with a std of 0.08045998796511386\n"
     ]
    }
   ],
   "source": [
    "abeta_neg_tau_neg_cn_df = GM_density_tau_amyloid_suvr_diag_df[(GM_density_tau_amyloid_suvr_diag_df['tau.global.SUVR_tau']<1.3)&(GM_density_tau_amyloid_suvr_diag_df['amyloid.global.SUVR.status_abeta']=='Ab.neg')&(GM_density_tau_amyloid_suvr_diag_df['Diag']=='CN')]\n",
    "abeta_neg_tau_neg_cn_df['Hippocampus Volume'] = abeta_neg_tau_neg_cn_df.apply(lambda row: calc_hipp_vol(row),axis=1)\n",
    "mean_neg_GM_Vol = abeta_neg_tau_neg_cn_df['Hippocampus Volume'].mean()\n",
    "std_neg_GM_Vol = abeta_neg_tau_neg_cn_df['Hippocampus Volume'].std()\n",
    "print('Of A-/T- subjects, the mean total GM volume was {} with a std of {}'.format(mean_neg_GM_Vol, std_neg_GM_Vol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "def sex_to_OH(row, key):\n",
    "    if row[key] == 'M':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "out_GM_volume_df_train['Sex OH'] = out_GM_volume_df_train.apply(lambda row: sex_to_OH(row, \"Sex_x\"), axis=1)\n",
    "out_GM_volume_df_test['Sex OH'] = out_GM_volume_df_test.apply(lambda row: sex_to_OH(row, \"Sex_x\"), axis=1)\n",
    "tau_abeta_AV45_overlap_suvr_df['Sex OH'] = tau_abeta_AV45_overlap_suvr_df.apply(lambda row: sex_to_OH(row, 'Sex'), axis=1)\n",
    "tau_abeta_FBB_overlap_suvr_df['Sex OH'] = tau_abeta_FBB_overlap_suvr_df.apply(lambda row: sex_to_OH(row, \"Sex\"), axis=1)\n",
    "\n",
    "\n",
    "# NEURODEGENERATION\n",
    "X_n = out_GM_volume_df_train[['tau.global.SUVR_tau','Age_x','Sex OH','APOE A1_x','APOE A2_x']].values\n",
    "\n",
    "keys = ['SUVR.Schaefer200.ROI.idx.{}'.format(i) for i in range(1,201)]\n",
    "keys.extend(['Age_x','Sex OH','APOE A1_x','APOE A2_x'])\n",
    "X_n = out_GM_volume_df_train[keys].values\n",
    "\n",
    "y_n = out_GM_volume_df_train[['N+']].values.flatten()\n",
    "\n",
    "# AMYLOID\n",
    "X_a = tau_abeta_AV45_overlap_suvr_df[['tau.global.SUVR_tau','Age','Sex OH','APOE A1','APOE A2']].values\n",
    "\n",
    "keys = ['SUVR.DKT.ROI.idx.{}_tau'.format(i) for i in dktLabels]\n",
    "keys.extend(['Age','Sex OH','APOE A1','APOE A2'])\n",
    "X_a = tau_abeta_AV45_overlap_suvr_df[keys].values\n",
    "\n",
    "y_a = tau_abeta_AV45_overlap_suvr_df[['A+']].values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC for max_depth=1 is 0.8277783304236775\n",
      "Mean AUC for max_depth=2 is 0.8198885080039849\n",
      "Mean AUC for max_depth=3 is 0.8136749029575661\n",
      "Mean AUC for max_depth=4 is 0.8173401843532867\n",
      "Mean AUC for max_depth=5 is 0.817958159321722\n"
     ]
    }
   ],
   "source": [
    "# XGBOOST sucks\n",
    "# Hyperparameter search for best max_depth\n",
    "for i in range(1,6):\n",
    "    gbc = XGBRegressor(random_state=1995, max_depth=i,objective=\"binary:logistic\", verbosity=0)\n",
    "    cv_results = cross_validate(gbc, X_n, y_n, cv=5, scoring='roc_auc')\n",
    "    mean_auc = np.mean(cv_results['test_score'])\n",
    "    print('Mean AUC for max_depth={} is {}'.format(i,mean_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC for max_depth=1 is 0.8319336033836441\n",
      "Mean AUC for max_depth=2 is 0.8231135526618658\n",
      "Mean AUC for max_depth=3 is 0.8269927342628215\n",
      "Mean AUC for max_depth=4 is 0.8140701726623043\n",
      "Mean AUC for max_depth=5 is 0.8141475158385673\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter search for best max_depth\n",
    "for i in range(1,6):\n",
    "    gbc = GradientBoostingClassifier(random_state=1995, max_depth=i)\n",
    "    cv_results = cross_validate(gbc, X_n, y_n, cv=5, scoring='roc_auc')\n",
    "    mean_auc = np.mean(cv_results['test_score'])\n",
    "    print('Mean AUC for max_depth={} is {}'.format(i,mean_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC for n_esimators=50 is 0.8184692199505662\n",
      "Mean AUC for n_esimators=100 is 0.8319336033836441\n",
      "Mean AUC for n_esimators=150 is 0.8345224247257021\n",
      "Mean AUC for n_esimators=200 is 0.8337323288996306\n",
      "Mean AUC for n_esimators=250 is 0.8331150049872305\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter search for best number of estimators\n",
    "for i in [50,100,150,200,250]:\n",
    "    gbc = GradientBoostingClassifier(random_state=1995, max_depth=1, n_estimators=i)\n",
    "    cv_results = cross_validate(gbc, X_n, y_n, cv=5, scoring='roc_auc')\n",
    "    mean_auc = np.mean(cv_results['test_score'])\n",
    "    print('Mean AUC for n_esimators={} is {}'.format(i,mean_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC for max_depth=1 is 0.7862319711538461\n",
      "Mean AUC for max_depth=2 is 0.7919146634615384\n",
      "Mean AUC for max_depth=3 is 0.7853365384615385\n",
      "Mean AUC for max_depth=4 is 0.7817848557692308\n",
      "Mean AUC for max_depth=5 is 0.7816334134615385\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter search for best max_depth\n",
    "for i in range(1,6):\n",
    "    gbc = GradientBoostingClassifier(random_state=1995, max_depth=i)\n",
    "    cv_results = cross_validate(gbc, X_a, y_a, cv=5, scoring='roc_auc')\n",
    "    mean_auc = np.mean(cv_results['test_score'])\n",
    "    print('Mean AUC for max_depth={} is {}'.format(i,mean_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC for n_estimators=50 is 0.78153125\n",
      "Mean AUC for n_estimators=100 is 0.7919146634615384\n",
      "Mean AUC for n_estimators=150 is 0.7900120192307692\n",
      "Mean AUC for n_estimators=200 is 0.7943557692307693\n",
      "Mean AUC for n_estimators=250 is 0.795890625\n",
      "Mean AUC for n_estimators=300 is 0.7952127403846154\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter search for best max_depth\n",
    "for i in [50,100,150,200,250,300]:\n",
    "    gbc = GradientBoostingClassifier(random_state=1995, max_depth=2, n_estimators=i)\n",
    "    cv_results = cross_validate(gbc, X_a, y_a, cv=5, scoring='roc_auc')\n",
    "    mean_auc = np.mean(cv_results['test_score'])\n",
    "    print('Mean AUC for n_estimators={} is {}'.format(i,mean_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time was 9.441375732421875e-05\n",
      "Prediction time was 0.0006780624389648438\n",
      "Prediction time was 0.0001811981201171875\n",
      "Prediction time was 0.0009045600891113281\n"
     ]
    }
   ],
   "source": [
    "# Run this for multiple runs over different seeds for CI calc\n",
    "\n",
    "orig_seed = 1995\n",
    "\n",
    "all_vars = [\"auc_a_lr_test\",\"f1_a_lr_test\",\"precision_a_lr_test\",\"recall_a_lr_test\",\"auc_a_gb_test\",\"f1_a_gb_test\",\"precision_a_gb_test\",\"recall_a_gb_test\",\"auc_n_lr_test\",\"f1_n_lr_test\",\"precision_n_lr_test\",\"recall_n_lr_test\",\"auc_n_gb_test\",\"f1_n_gb_test\",\"precision_n_gb_test\",\"recall_n_gb_test\",\"acc_atn_lr\",\"f1_atn_lr\",\"prec_atn_lr\",\"recall_atn_lr\",\"acc_atn_gb\",\"f1_atn_gb\",\"prec_atn_gb\",\"recall_atn_gb\"]\n",
    "all_scores = {}\n",
    "for v in all_vars:\n",
    "    all_scores[v] = []\n",
    "\n",
    "for seed in range(10,11):\n",
    "    # AMYLOID\n",
    "    \n",
    "    # Hyperparameter search for best cutoff for F1\n",
    "    best_cutoff_scorer = metrics.make_scorer(best_cutoff, needs_threshold=True)\n",
    "    gbc = GradientBoostingClassifier(random_state=seed, max_depth=2, n_estimators=250)\n",
    "    cv_results = cross_validate(gbc, X_a, y_a, cv=5, scoring=best_cutoff_scorer)\n",
    "    mean_cutoff_gb_a = np.mean(cv_results['test_score'])\n",
    "    #print('Optimal cutoff for GBC is {}'.format(mean_cutoff_gb_a))\n",
    "\n",
    "    best_cutoff_scorer = metrics.make_scorer(best_cutoff, needs_threshold=True)\n",
    "    lr = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "    cv_results = cross_validate(lr, X_a, y_a, cv=5, scoring=best_cutoff_scorer)\n",
    "    mean_cutoff_lr_a = np.mean(cv_results['test_score'])\n",
    "    #print('Optimal cutoff for LR is {}'.format(mean_cutoff_lr_a))\n",
    "    \n",
    "    # Evaluate Amyloid\n",
    "    out_abeta_df_test = tau_abeta_FBB_overlap_suvr_df.iloc[test_ids_amyloid_overlap_gmdensity_indices]\n",
    "\n",
    "    keys = ['SUVR.DKT.ROI.idx.{}_tau'.format(i) for i in dktLabels]\n",
    "    keys.extend(['Age','Sex OH','APOE A1','APOE A2'])\n",
    "    X_a_test = out_abeta_df_test[keys].values\n",
    "    y_a_test = out_abeta_df_test[['A+']].values.flatten()\n",
    "    \n",
    "    # Train final model\n",
    "    clf_lr_a = LogisticRegression(random_state=seed,max_iter=1000).fit(X_a,y_a)\n",
    "    clf_gb_a = GradientBoostingClassifier(random_state=seed, max_depth=1, n_estimators=150).fit(X_a,y_a)\n",
    "\n",
    "    #print('Performance LR:')\n",
    "    y_hat_a_lr_test, auc_a_lr_test, f1_a_lr_test, precision_a_lr_test, recall_a_lr_test = evaluate_performance(clf_lr_a, X_a, y_a, X_a_test, y_a_test, mean_cutoff_lr_a, _print=False)\n",
    "    #print('\\n\\nPerformance GB:')\n",
    "    y_hat_a_gb_test, auc_a_gb_test, f1_a_gb_test, precision_a_gb_test, recall_a_gb_test = evaluate_performance(clf_gb_a, X_a, y_a, X_a_test, y_a_test, mean_cutoff_gb_a, _print=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # NEURODEGENERATION\n",
    "    \n",
    "    # Hyperparameter search for best cutoff for F1\n",
    "    best_cutoff_scorer = metrics.make_scorer(best_cutoff, needs_threshold=True)\n",
    "    gbc = GradientBoostingClassifier(random_state=seed, max_depth=1, n_estimators=150)\n",
    "    cv_results = cross_validate(gbc, X_n, y_n, cv=5, scoring=best_cutoff_scorer)\n",
    "    mean_cutoff_gb_n = np.mean(cv_results['test_score'])\n",
    "    #print('Optimal cutoff for GBC is {}'.format(mean_cutoff_gb_n))\n",
    "\n",
    "    best_cutoff_scorer = metrics.make_scorer(best_cutoff, needs_threshold=True)\n",
    "    lr = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "    cv_results = cross_validate(lr, X_n, y_n, cv=5, scoring=best_cutoff_scorer)\n",
    "    mean_cutoff_lr_n = np.mean(cv_results['test_score'])\n",
    "    #print('Optimal cutoff for LR is {}'.format(mean_cutoff_lr_n))\n",
    "    \n",
    "    # Train final model\n",
    "    clf_lr_n = LogisticRegression(random_state=seed,max_iter=1000).fit(X_n,y_n)\n",
    "    clf_gb_n = GradientBoostingClassifier(random_state=seed, max_depth=1, n_estimators=150).fit(X_n,y_n)\n",
    "    \n",
    "    # Evaluate Neurodegeneration\n",
    "    keys = ['SUVR.Schaefer200.ROI.idx.{}'.format(i) for i in range(1,201)]\n",
    "    keys.extend(['Age_x','Sex OH','APOE A1_x','APOE A2_x'])\n",
    "    #X_n_test = out_GM_volume_df_test[['tau.global.SUVR','Age_x','Sex OH','APOE A1_x','APOE A2_x']].values\n",
    "    X_n_test = out_GM_volume_df_test[keys].values\n",
    "    y_n_test = out_GM_volume_df_test[['N+']].values.flatten()\n",
    "\n",
    "    #print('Performance LR:')\n",
    "    y_hat_n_lr_test, auc_n_lr_test, f1_n_lr_test, precision_n_lr_test, recall_n_lr_test = evaluate_performance(clf_lr_n, X_n, y_n, X_n_test, y_n_test, mean_cutoff_lr_n, _print=False)\n",
    "    #print('\\n\\nPerformance GB:')\n",
    "    y_hat_n_gb_test, auc_n_gb_test, f1_n_gb_test, precision_n_gb_test, recall_n_gb_test = evaluate_performance(clf_gb_n, X_n, y_n, X_n_test, y_n_test, mean_cutoff_gb_n, _print=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ATN CLASSIFICATION\n",
    "    \n",
    "    # Logistic Regression\n",
    "    gt = classify_atn(y_n_test,y_a_test)\n",
    "    pred = classify_atn(y_hat_n_lr_test,y_hat_a_lr_test)\n",
    "    acc_atn_lr = metrics.accuracy_score(gt,pred)\n",
    "    f1_atn_lr = metrics.f1_score(gt,pred,average='macro')\n",
    "    prec_atn_lr = metrics.precision_score(gt,pred,average='macro')\n",
    "    recall_atn_lr = metrics.recall_score(gt,pred,average='macro')\n",
    "    #print('For ATN classification with LR, accuracy was {} and F1 was {} (Prec: {} | Rec: {})'.format(acc_atn_lr,f1_atn_lr,prec_atn_lr,recall_atn_lr))\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    gt = classify_atn(y_n_test,y_a_test)\n",
    "    pred = classify_atn(y_hat_n_gb_test,y_hat_a_gb_test)\n",
    "    acc_atn_gb = metrics.accuracy_score(gt,pred)\n",
    "    f1_atn_gb = metrics.f1_score(gt,pred,average='macro')\n",
    "    prec_atn_gb = metrics.precision_score(gt,pred,average='macro')\n",
    "    recall_atn_gb = metrics.recall_score(gt,pred,average='macro')\n",
    "    #print('For ATN classification with GB, accuracy was {} and F1 was {} (Prec: {} | Rec: {})'.format(acc_atn_gb,f1_atn_gb,prec_atn_gb,recall_atn_gb))\n",
    "    \n",
    "    for v in all_vars:\n",
    "        all_scores[v].append(eval(v))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_a_lr_test       : 0.87 ± 0.00\n",
      "f1_a_lr_test        : 0.69 ± 0.00\n",
      "precision_a_lr_test : 0.69 ± 0.00\n",
      "recall_a_lr_test    : 0.70 ± 0.00\n",
      "auc_a_gb_test       : 0.85 ± 0.00\n",
      "f1_a_gb_test        : 0.70 ± 0.00\n",
      "precision_a_gb_test : 0.77 ± 0.00\n",
      "recall_a_gb_test    : 0.63 ± 0.00\n",
      "auc_n_lr_test       : 0.82 ± 0.00\n",
      "f1_n_lr_test        : 0.54 ± 0.00\n",
      "precision_n_lr_test : 0.49 ± 0.00\n",
      "recall_n_lr_test    : 0.60 ± 0.00\n",
      "auc_n_gb_test       : 0.81 ± 0.00\n",
      "f1_n_gb_test        : 0.51 ± 0.00\n",
      "precision_n_gb_test : 0.52 ± 0.00\n",
      "recall_n_gb_test    : 0.50 ± 0.00\n",
      "acc_atn_lr          : 0.69 ± 0.00\n",
      "f1_atn_lr           : 0.48 ± 0.00\n",
      "prec_atn_lr         : 0.46 ± 0.00\n",
      "recall_atn_lr       : 0.55 ± 0.00\n",
      "acc_atn_gb          : 0.70 ± 0.00\n",
      "f1_atn_gb           : 0.52 ± 0.00\n",
      "prec_atn_gb         : 0.54 ± 0.00\n",
      "recall_atn_gb       : 0.53 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "for v in all_vars:\n",
    "    calc_mean_std(v, all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_a_lr_test       : 0.87 [0.87, 0.87]\n",
      "f1_a_lr_test        : 0.69 [0.69, 0.69]\n",
      "precision_a_lr_test : 0.69 [0.69, 0.69]\n",
      "recall_a_lr_test    : 0.70 [0.70, 0.70]\n",
      "auc_a_gb_test       : 0.85 [0.85, 0.85]\n",
      "f1_a_gb_test        : 0.70 [0.70, 0.70]\n",
      "precision_a_gb_test : 0.77 [0.77, 0.77]\n",
      "recall_a_gb_test    : 0.63 [0.63, 0.63]\n",
      "auc_n_lr_test       : 0.82 [0.82, 0.82]\n",
      "f1_n_lr_test        : 0.54 [0.54, 0.54]\n",
      "precision_n_lr_test : 0.49 [0.49, 0.49]\n",
      "recall_n_lr_test    : 0.60 [0.60, 0.60]\n",
      "auc_n_gb_test       : 0.81 [0.81, 0.81]\n",
      "f1_n_gb_test        : 0.51 [0.51, 0.51]\n",
      "precision_n_gb_test : 0.52 [0.52, 0.52]\n",
      "recall_n_gb_test    : 0.50 [0.50, 0.50]\n",
      "acc_atn_lr          : 0.69 [0.69, 0.69]\n",
      "f1_atn_lr           : 0.48 [0.48, 0.48]\n",
      "prec_atn_lr         : 0.46 [0.46, 0.46]\n",
      "recall_atn_lr       : 0.55 [0.55, 0.55]\n",
      "acc_atn_gb          : 0.70 [0.70, 0.70]\n",
      "f1_atn_gb           : 0.52 [0.52, 0.52]\n",
      "prec_atn_gb         : 0.54 [0.54, 0.54]\n",
      "recall_atn_gb       : 0.53 [0.53, 0.53]\n"
     ]
    }
   ],
   "source": [
    "for v in all_vars:\n",
    "    calc_CI(v, all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal cutoff for GBC is -0.7101771961530015\n",
      "Optimal cutoff for LR is -0.7875295711258137\n"
     ]
    }
   ],
   "source": [
    "# AMYLOID\n",
    "# Hyperparameter search for best cutoff for F1\n",
    "best_cutoff_scorer = metrics.make_scorer(best_cutoff, needs_threshold=True)\n",
    "gbc = GradientBoostingClassifier(random_state=1995, max_depth=2, n_estimators=250)\n",
    "cv_results = cross_validate(gbc, X_a, y_a, cv=5, scoring=best_cutoff_scorer)\n",
    "mean_cutoff_gb_a = np.mean(cv_results['test_score'])\n",
    "print('Optimal cutoff for GBC is {}'.format(mean_cutoff_gb_a))\n",
    "\n",
    "best_cutoff_scorer = metrics.make_scorer(best_cutoff, needs_threshold=True)\n",
    "lr = LogisticRegression(random_state=1995, max_iter=1000)\n",
    "cv_results = cross_validate(lr, X_a, y_a, cv=5, scoring=best_cutoff_scorer)\n",
    "mean_cutoff_lr_a = np.mean(cv_results['test_score'])\n",
    "print('Optimal cutoff for LR is {}'.format(mean_cutoff_lr_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal cutoff for GBC is -1.0265091007812368\n",
      "Optimal cutoff for LR is -0.9537148966513527\n"
     ]
    }
   ],
   "source": [
    "# NEURODEGENERATION\n",
    "# Hyperparameter search for best cutoff for F1\n",
    "best_cutoff_scorer = metrics.make_scorer(best_cutoff, needs_threshold=True)\n",
    "gbc = GradientBoostingClassifier(random_state=1995, max_depth=1, n_estimators=150)\n",
    "cv_results = cross_validate(gbc, X_n, y_n, cv=5, scoring=best_cutoff_scorer)\n",
    "mean_cutoff_gb_n = np.mean(cv_results['test_score'])\n",
    "print('Optimal cutoff for GBC is {}'.format(mean_cutoff_gb_n))\n",
    "\n",
    "best_cutoff_scorer = metrics.make_scorer(best_cutoff, needs_threshold=True)\n",
    "lr = LogisticRegression(random_state=1995, max_iter=1000)\n",
    "cv_results = cross_validate(lr, X_n, y_n, cv=5, scoring=best_cutoff_scorer)\n",
    "mean_cutoff_lr_n = np.mean(cv_results['test_score'])\n",
    "print('Optimal cutoff for LR is {}'.format(mean_cutoff_lr_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model\n",
    "clf_lr_n = LogisticRegression(random_state=1995,max_iter=1000).fit(X_n,y_n)\n",
    "clf_gb_n = GradientBoostingClassifier(random_state=1995, max_depth=1, n_estimators=150).fit(X_n,y_n)\n",
    "\n",
    "clf_lr_a = LogisticRegression(random_state=1995,max_iter=1000).fit(X_a,y_a)\n",
    "clf_gb_a = GradientBoostingClassifier(random_state=1995, max_depth=1, n_estimators=150).fit(X_a,y_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance LR:\n",
      "Prediction time was 0.00037360191345214844\n",
      "Train\n",
      "F1: 0.6588921282798834\n",
      "AUC: 0.7524398797595191\n",
      "---\n",
      "Test\n",
      "F1: 0.5287356321839081 | Precision: 0.5 | Recall: 0.5609756097560976\n",
      "AUC: 0.7911494085000731\n",
      "\n",
      "\n",
      "Performance GB:\n",
      "Prediction time was 0.0009856224060058594\n",
      "Train\n",
      "F1: 0.712166172106825\n",
      "AUC: 0.7829659318637275\n",
      "---\n",
      "Test\n",
      "F1: 0.5263157894736842 | Precision: 0.46296296296296297 | Recall: 0.6097560975609756\n",
      "AUC: 0.7444136118007886\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Neurodegeneration\n",
    "keys = ['SUVR.Schaefer200.ROI.idx.{}'.format(i) for i in range(1,201)]\n",
    "keys.extend(['Age_x','Sex OH','APOE A1_x','APOE A2_x'])\n",
    "#X_n_test = out_GM_volume_df_test[['tau.global.SUVR','Age_x','Sex OH','APOE A1_x','APOE A2_x']].values\n",
    "X_n_test = out_GM_volume_df_test[keys].values\n",
    "y_n_test = out_GM_volume_df_test[['N+']].values.flatten()\n",
    "\n",
    "print('Performance LR:')\n",
    "y_hat_n_lr_test = evaluate_performance(clf_lr_n, X_n, y_n, X_n_test, y_n_test, mean_cutoff_lr_n)\n",
    "print('\\n\\nPerformance GB:')\n",
    "y_hat_n_gb_test = evaluate_performance(clf_gb_n, X_n, y_n, X_n_test, y_n_test, mean_cutoff_gb_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_n = pd.DataFrame({'gt':y_n_test ,'pred':clf_lr_n.decision_function(X_n_test)})\n",
    "lr_n.to_csv('../data/lr_neurodegen.csv')\n",
    "\n",
    "gb_n = pd.DataFrame({'gt':y_n_test ,'pred':clf_gb_n.decision_function(X_n_test)})\n",
    "gb_n.to_csv('../data/gb_neurodegen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance LR:\n",
      "Prediction time was 0.0003578662872314453\n",
      "Train\n",
      "F1: 0.7062146892655367\n",
      "AUC: 0.767746913580247\n",
      "---\n",
      "Test\n",
      "F1: 0.6928104575163399 | Precision: 0.6883116883116883 | Recall: 0.6973684210526315\n",
      "AUC: 0.8672157492969064\n",
      "\n",
      "\n",
      "Performance GB:\n",
      "Prediction time was 0.0007977485656738281\n",
      "Train\n",
      "F1: 0.7813411078717202\n",
      "AUC: 0.8211111111111111\n",
      "---\n",
      "Test\n",
      "F1: 0.7236842105263158 | Precision: 0.7236842105263158 | Recall: 0.7236842105263158\n",
      "AUC: 0.8502410606669345\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Amyloid\n",
    "out_abeta_df_test = tau_abeta_FBB_overlap_suvr_df.iloc[test_ids_amyloid_overlap_gmdensity_indices]\n",
    "#out_abeta_df_test = tau_abeta_FBB_overlap_suvr_df\n",
    "\n",
    "keys = ['SUVR.DKT.ROI.idx.{}_tau'.format(i) for i in dktLabels]\n",
    "keys.extend(['Age','Sex OH','APOE A1','APOE A2'])\n",
    "X_a_test = out_abeta_df_test[keys].values\n",
    "y_a_test = out_abeta_df_test[['A+']].values.flatten()\n",
    "\n",
    "print('Performance LR:')\n",
    "y_hat_a_lr_test = evaluate_performance(clf_lr_a, X_a, y_a, X_a_test, y_a_test, mean_cutoff_lr_a)\n",
    "print('\\n\\nPerformance GB:')\n",
    "y_hat_a_gb_test = evaluate_performance(clf_gb_a, X_a, y_a, X_a_test, y_a_test, mean_cutoff_gb_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_a = pd.DataFrame({'gt':y_a_test ,'pred':clf_lr_a.decision_function(X_a_test)})\n",
    "lr_a.to_csv('../data/lr_amyloid.csv')\n",
    "\n",
    "gb_a = pd.DataFrame({'gt':y_a_test ,'pred':clf_gb_a.decision_function(X_a_test)})\n",
    "gb_a.to_csv('../data/gb_amyloid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure test sets are looking at same subjects (last three columns are sex and APOE status)\n",
    "assert np.all(X_a_test[:,-3:]==X_n_test[:,-3:]), 'Test sets are different between N and A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb Cell 86'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241494d5f536572766572227d/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb#ch0000083vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# Logistic Regression\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241494d5f536572766572227d/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb#ch0000083vscode-remote?line=1'>2</a>\u001b[0m gt \u001b[39m=\u001b[39m classify_atn(y_n_test,y_a_test)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241494d5f536572766572227d/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb#ch0000083vscode-remote?line=2'>3</a>\u001b[0m pred \u001b[39m=\u001b[39m classify_atn(y_hat_n_lr_test,y_hat_a_lr_test)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241494d5f536572766572227d/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb#ch0000083vscode-remote?line=3'>4</a>\u001b[0m acc \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39maccuracy_score(gt,pred)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241494d5f536572766572227d/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb#ch0000083vscode-remote?line=4'>5</a>\u001b[0m f1 \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mf1_score(gt,pred,average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb Cell 15'\u001b[0m in \u001b[0;36mclassify_atn\u001b[0;34m(n, a)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241494d5f536572766572227d/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb#ch0000014vscode-remote?line=3'>4</a>\u001b[0m n_ \u001b[39m=\u001b[39m n[i]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241494d5f536572766572227d/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb#ch0000014vscode-remote?line=4'>5</a>\u001b[0m a_ \u001b[39m=\u001b[39m a[i]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241494d5f536572766572227d/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb#ch0000014vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m n_ \u001b[39mand\u001b[39;00m a_:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241494d5f536572766572227d/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb#ch0000014vscode-remote?line=6'>7</a>\u001b[0m     out\u001b[39m.\u001b[39mappend(\u001b[39m3\u001b[39m) \u001b[39m# N+|A+\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241494d5f536572766572227d/home/guests/paul_hager/Projects/ATN_Classification/Baseline/BaselineClassifiers.ipynb#ch0000014vscode-remote?line=7'>8</a>\u001b[0m \u001b[39melif\u001b[39;00m n_ \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m a_:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "gt = classify_atn(y_n_test,y_a_test)\n",
    "pred = classify_atn(y_hat_n_lr_test,y_hat_a_lr_test)\n",
    "acc = metrics.accuracy_score(gt,pred)\n",
    "f1 = metrics.f1_score(gt,pred,average='macro')\n",
    "prec = metrics.precision_score(gt,pred,average='macro')\n",
    "recall = metrics.recall_score(gt,pred,average='macro')\n",
    "print('For ATN classification with LR, accuracy was {} and F1 was {} (Prec: {} | Rec: {})'.format(acc,f1,prec,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ATN classification with GB, accuracy was 0.7101449275362319 and F1 was 0.5324228781831122 (Prec: 0.5623778998778999 | Rec: 0.5453830891330891)\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "gt = classify_atn(y_n_test,y_a_test)\n",
    "pred = classify_atn(y_hat_n_gb_test,y_hat_a_gb_test)\n",
    "acc = metrics.accuracy_score(gt,pred)\n",
    "f1 = metrics.f1_score(gt,pred,average='macro')\n",
    "prec = metrics.precision_score(gt,pred,average='macro')\n",
    "recall = metrics.recall_score(gt,pred,average='macro')\n",
    "print('For ATN classification with GB, accuracy was {} and F1 was {} (Prec: {} | Rec: {})'.format(acc,f1,prec,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0\n",
      "---\n",
      "0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,1,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,1,1,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,1,1,0\n"
     ]
    }
   ],
   "source": [
    "n_pred_gb = [str(int(i)) for i in y_hat_n_gb_test.tolist()]\n",
    "n_pred_lr = [str(int(i)) for i in y_hat_n_lr_test.tolist()]\n",
    "print(','.join(n_pred_gb))\n",
    "print('---')\n",
    "print(','.join(n_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,0,0,1,0,1,0,0,1,0,1,1,0,1,0,0,0,0,1,0,1,1,1,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,0,1,1,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0,1,1,1,1,0,0,1,1,0,0,1,1,1,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,1,0,1,1,1,1,0,0,0,0,0,0,1,1,1,0,0,1,1,0,0,0,1,0,0,0,1,0,0,1,1,0,0,1,1,1,1,0,0,1,0,0,0,0,1,1,1,0\n",
      "---\n",
      "0,1,0,1,0,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,0,0,1,0,1,0,1,1,0,1,1,0,1,1,0,0,0,1,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,1,1,1,1,0,1,1,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0,1,0,1,1,1,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,1,1,1,0,0,1,1,0,1,1,1,1,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,1,1,1,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,1,1,0,1,1,1,1,0,0,0,0,0,0,1,0,1,1,0\n"
     ]
    }
   ],
   "source": [
    "a_pred_gb = [str(int(i)) for i in y_hat_a_gb_test.tolist()]\n",
    "a_pred_lr = [str(int(i)) for i in y_hat_a_lr_test.tolist()]\n",
    "print(','.join(a_pred_gb))\n",
    "print('---')\n",
    "print(','.join(a_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 207 test subjects, 142 were ATN classified correctly, 52 had one predicted biomarker incorrect and 13 had both biomarkers incorrect\n",
      "Abeta miss only: 34\n",
      "N miss only: 18\n"
     ]
    }
   ],
   "source": [
    "# Accuracy is a bad measure when you have such a small minority \n",
    "correct = 0\n",
    "miss = 0\n",
    "double_miss = 0\n",
    "abeta_miss = 0\n",
    "n_miss = 0\n",
    "\n",
    "out_a = y_hat_a_lr_test\n",
    "out_n = y_hat_n_lr_test\n",
    "\n",
    "for i in range(len(y_a_test)):\n",
    "    if (y_a_test[i]==out_a[i]) and (y_n_test[i]==out_n[i]):\n",
    "        correct += 1\n",
    "    elif (y_a_test[i]!=out_a[i]) and (y_n_test[i]!=out_n[i]):\n",
    "        double_miss += 1\n",
    "    else:\n",
    "        if (y_a_test[i]!=out_a[i]):\n",
    "            abeta_miss += 1\n",
    "        else:\n",
    "            n_miss += 1\n",
    "        miss += 1\n",
    "\n",
    "print(\"Out of {} test subjects, {} were ATN classified correctly, {} had one predicted biomarker incorrect and {} had both biomarkers incorrect\".format(len(y_a_test),correct,miss,double_miss))\n",
    "print(\"Abeta miss only: {}\".format(abeta_miss))\n",
    "print(\"N miss only: {}\".format(n_miss))        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4a36f6bd247a45e88e4b7f3df039338fdaf42e1a06d7fd0f82a1a370a4df456"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
